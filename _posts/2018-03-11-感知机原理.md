---
layout: post
title: "感知机原理"
date: "2018-03-11"
categories: 
  - "数学"
---

# 感知机模型

## 感知机的思想

感知机的思想很简单，比如我们在一个平台上有很多男孩女孩，感知机的模型就是尝试找到一条直线，能够把所有的男孩女孩隔开。放在三维或者更高维度的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果找不到这么一条直线，意味着线性不可分，使用感知机最大的一个前提是，数据是线性可分的。

## 数学描述感知机

如果我们有m个样本，每个样本对应n维特征和一个二元类别输出，如下

$$(x\_1^{(1)},x\_2^{(1)},\\cdots,x\_n^{(1)},y\_1),(x\_1^{(2)},x\_2^{(2)},\\cdots,x\_n^{(2)},y\_2),\\cdots,(x\_1^{(m)},x\_2^{(m)},\\cdots,x\_n^{(m)},y\_m)$$

我们的目标是找到这么一个超平面，即

$$\\theta\_0+\\theta\_1x\_1+\\cdots+\\theta\_nx\_n=0$$

让其中一种类别的样品都满足 $latex \\theta\_0+\\theta\_1x\_1+\\cdots+\\theta\_nx\_n >0$，让另一种类别的样品都满足 $latex \\theta\_0+\\theta\_1x\_1+\\cdots+\\theta\_nx\_n<0$，从而得到线性可分，如果数据线性可分，这样的超平面一般不是唯一的，也就是说感知机模型可以有多个解。

为了简化超平面这个写法，我们增加一个特征 $latex x\_0=1$,这样超平面为 $latex \\sum\\limits\_{i=0}^n \\theta\_ix\_i=0$,进一步用向量表示为 $latex \\theta\\cdot x=0$,其中$latex \\theta为(n+1)\\times 1的向量，x为\\theta为(n+1)\\times 1的向量，\\cdot 为内积$，以后我们都用向量来表示超平面。

而感知机的模型可以定义为 $latex y=sign(\\theta\\cdot x)$，其中 $$sign(x)=\\left\\{\\begin{align}&-1&x<0\\\\&1&x\\ge 0\\end{align}\\right.$$

# 感知机模型损失函数

## 损失函数构思

- 取值：
    - 将满足$latex \\theta\\cdot x>0 $的样本类别的输出值为1
    - 将满足$latex \\theta\\cdot x<0 $的样本类别的输出值为-1
- 损失函数
    - 正确分类的样本，$latex y\\theta\\cdot x>0$
    - 错误分类的样本,    $latex y\\theta\\cdot x<0$
- 损失函数的目标
    - 误分类的所有样品，到超平面的距离之和最小。
- 构造损失函数
    - 由于 $latex y\\theta\\cdot x<0$，对于每一个误分类的样本i，到超平面的距离是
        - $latex -\\frac{y^{(i)}\\theta\\cdot x^{(i)}}{||\\theta||\_2}$
            - 其中 $latex ||\\theta||\_2$为L2范数，也就是$latex \\theta$的各个值的平方和。
    - 我们假设所有误分类的的的集合为M，则所有误分类的样本到超平面的距离之和为
        - $latex -\\sum\\limits\_{x\_i\\in M}\\frac{y^{(i)}\\theta\\cdot x^{(i)}}{||\\theta||\_2}$
- 精简
    - 我们发现分子和分母都含有 $latex \\theta$ ,分子和分母存在固定的倍数关系，那么我们可以固定分子或者分母为1，然后求分子自己或者分母的导数的最小化作为损失函数。在感知器模型中，我们采取的是保留分子，精简化的感知器模型的损失函数如下：
        - $latex J(\\theta)=-\\sum\\limits\_{x\_i\\in M}y^{(i)}\\theta\\cdot x^{(i)}$

# 感知器模型损失函数的优化方法

感知器的损失函数为 $latex J(\\theta)=-\\sum\\limits\_{x\_i\\in M}y^{(i)}\\theta\\cdot x^{(i)}$，可以用梯度下降法，但不能用普通的梯度下降法，原因在于我们在损失函数中有限定，只有误分类的M集合的样本才能参与损失函数的优化，所以只能采取随机梯度下降或者小批量梯度下降，

损失函数基于$latex \\theta$向量的偏导数为

$latex \\frac{\\partial}{\\partial\\theta}J(\\theta)=-\\sum\\limits\_{x\_i\\in M} y^{(i)}x^{(i)}$

$latex \\theta$的梯度下降迭代公式为

$latex \\theta=\\theta+\\alpha\\sum\\limits\_{x\_i\\in M} y^{(i)}x^{(i)}$

由于我们采用的是随机梯度下降，所以每次仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的$latex \\theta$向量的梯度下降迭代公式为

$latex \\theta=\\theta+\\alpha y^{(i)}x^{(i)}$

其中 $latex \\alpha 为步长，y^{(i)}为样本输出值1或者-1，x^{(i)}为(n+1)\\times 1的向量$

# 感知器模型的算法总结

- 输入：
    - 算法输入为m个样本，每个样本对应n维特征和一个二元类别输出1或者-1，如下
        - $latex (x\_1^{(1)},x\_2^{(1)},\\cdots,x\_n^{(1)},y\_1),(x\_1^{(2)},x\_2^{(2)},\\cdots,x\_n^{(2)},y\_2),\\cdots,(x\_1^{(m)},x\_2^{(m)},\\cdots,x\_n^{(m)},y\_m)$
- 输出 ：
    - 分离超平面的模型系数 $latex \\theta $向量。
    - 使得误分类的样本到超平面的距离和最小。
        - 距离公式
            - $latex \\frac{\\theta\\cdot\\vec{x}}{||\\theta||\_2}$
        - 定义值
            - $latex \\theta\\cdot\\vec{x}>0的样本类别为1$
            - $latex \\theta\\cdot\\vec{x}<0的样本类别为-1$
        - 这个距离公式只是算误分类的，
        - 这时候用 $latex y^{(i)}$乘以 $latex \\theta\\cdot\\vec{x}$就比较好判断误分类
            - 正确分类的样本，值大于0
            - 错误分类的样本，值少于0
        - 当然，因为这个错误分类的结果为0，那么前面加上一个负号。就得到最终的公式
        - $latex J(\\theta)=-\\sum\\limits\_{x\_i\\in M}y^{(i)}\\theta\\cdot x^{(i)}$
- 执行步骤
    1. 初始化：
        - 定义所有的 $latex x\_0$为1
        - 设置 $latex \\theta$向量的初值。
            - 可以设置为0
        - 设置 $latex \\alpha$的初值。
            - 可以设置为1
    2. 在训练集选择一个误分类的点 $latex (x\_1^{(i)},x\_2^{(i)},\\cdots,x\_n^{(i)},y\_i)$，用向量表示是$latex (x^{(i)},y^{(i)})$,这个点应该满足 $latex y^{(i)}\\theta\\cdot x^{(i)}<0$
    3. 对 $latex \\theta$向量进行一次随机梯度下降的迭代，$$\\theta=\\theta+\\alpha y^{(i)}x^{(i)}$$
    4. 检查训练集里是否还有误分类的点，如果没有，算法结束，此时 $latex \\theta$即为最终结果，如果有，继续第2步。

# 感知机模型算法的对偶形式

感知器模型的的原始算法中，$latex \\theta=\\theta+\\alpha y^{(i)}x^{(i)}$,我们每次梯度下降都是选择一个样本来更新$latex \\theta $向量，最终经过若干次的迭代得到最终的结果，对于从来都没有误分类的样本，他被选择参与$latex \\theta$迭代的次数为0，对于被多次误分类而更新的样本j，它参与$latex \\theta$迭代的次数，我们设置为 $latex m\_j$,如果令$latex \\theta$向量初始值为0向量，那么我们的$latex \\theta $向量的表达式可以写成

$latex \\theta =\\alpha \\sum\\limits\_{j=1}^{m}m\_{j}y^{(j)}x^{(j)}$

- 其中 $latex m\_j$为样本 $latex (x^{(j)},y^{(j)})$在梯度下降到当前的这一步之前因误分类而更新的次数。
    - 每个样本的$latex (x^{(j)},y^{(j)})$的$latex m\_j$的初始值为0，每当此样品在某一次梯度下降中因误分类而更新时，$latex m\_j$的值增加1.

由于步长 $latex \\alpha$为常量，我们令 $latex \\beta\_j=\\alpha m\_j $,这样表达式如下

$latex \\theta = \\sum\\limits\_{j=1}^{m}\\beta\_j y^{(i)}x^{(i)}$

- 注意这里的$latex \\sum\\limits\_{j=1}^{m}$是计算了这个样本集合。
- 可以这样计算。
    - $latex y^{(j)}$是一个值
        - y 为一个向量
    - $latex x^{(j)}$是一个向量
        - x为一个矩阵。

在每一步判断误分类条件的地方，我们用 $latex y^{(i)}\\theta\\cdot x^{(i)}<0$的变种 $latex y^{(i)}\\sum\\limits\_{j=1}^{m}\\beta\_i y^{(j)}x^{(j)}\\cdot x^{(i)}<0$来判断误分类，就是将 $latex \\theta$代入得到这个公式。

## 小结：

原始的$latex \\theta$更新是如下 $latex \\theta = \\theta + \\alpha\\sum\\limits\_{x\_i \\in M}y^{(i)}x^{(i)}$

- 这里的M是误分类集合，

但因为这个只是误分类集合，所以用随机梯度来求，每次只取一个样本 $latex \\theta = \\theta + \\alpha y^{(i)}x^{(i)}$

文章中的$latex m\_j$，我是这么理解的，随机梯度，每次只取一个样本，太慢了，所以用这个，用所有的样本，这样就相当于批量梯度下降了。

$latex \\theta = \\alpha \\sum\\limits\_{j=1}^{m}m\_jy^{(j)}x^{(j)}$

又由于 $latex \\alpha$为常量，所以令 $latex \\beta\_i=\\alpha m\_j$，表达式可以写成如下形式

$latex \\theta = \\sum\\limits\_{j=1}^{m}\\beta\_j y^{(j)}x^{(j)}$

判断是否是误分类的，就是如下的形式了

$latex y^{(i)}\\sum\\limits\_{j=1}^{m}\\beta\_j y^{(j)}x^{(j)}\\bullet x^{(i)} < 0$

步骤：

1. 初始化数据
    - $latex x\_0$为1，
    - 设置步长a为1，
    - $latex \\beta$的初值为0
2. 计算内积形成的Gram形成的矩阵，为了以后方便运算而已，毕竟少很多运算。
3. 在训练集选择一个误分类的点 $latex (x^{(i)},y^{(i)})$,
    - 这个点满足$latex y^{(i)}\\sum\\limits\_{j=1}^{m}\\beta\_j y^{(j)}x^{(j)}\\bullet x^{(i)} <= 0$
4. 对 $latex \\beta$向量的第i个分量进行一次更新，$latex \\beta\_i=\\beta\_i+a$
5. 检查训练集是否还有其他误分类的点，
    - 如果没有，算法结束，结果为下式 $latex \\theta=\\sum\\limits\_{j=1}^{m}\\beta\_{j}y^{(j)}x^{(j)}$

如下是我在jupyder中运行的，但看起来这个对偶并不省时间，最省时间的反倒是我第一种算法，用numpy的矩阵的。

```
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

```

# 输入数据[¶](#输入数据)

In \[5\]:

```
# 我们先设置几个数据吧
x=np.array([[1,2],[2,2],[3,1],[10,8],[6,9],[1,1],[3,6],[4,4],[6,8],[7,6],[3,2],[7,8],[6,2],[9,6],[11,3],[10,6],[12,5],[2,6]])
y=np.array([-1,-1,-1,1,1,-1,-1,-1,1,1,-1,1,-1,1,1,1,1,-1])
y=y.reshape(len(y),1)
y_g_0=np.where(y>0)
for i in x[(np.where(y>0))[0]]:
    plt.plot(i[0],i[1],'x')
for i in x[(np.where(y<0))[0]]:
    plt.plot(i[0],i[1],'o') 

```

![no img](data:image/png;base64,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)

# 初始化数据

我们要求分开这几个数据，假设我们最终要求的函数为 y\=xθ

y\=xθ,我这里定义为相乘，设置x在增加一列，全是1，即设x0\=1

x0\=1 

In \[6\]:

```
x=np.insert(x,0,values=1,axis=1)
x

```

Out\[6\]:

```
array([[ 1,  1,  2],
       [ 1,  2,  2],
       [ 1,  3,  1],
       [ 1, 10,  8],
       [ 1,  6,  9],
       [ 1,  1,  1],
       [ 1,  3,  6],
       [ 1,  4,  4],
       [ 1,  6,  8],
       [ 1,  7,  6],
       [ 1,  3,  2],
       [ 1,  7,  8],
       [ 1,  6,  2],
       [ 1,  9,  6],
       [ 1, 11,  3],
       [ 1, 10,  6],
       [ 1, 12,  5],
       [ 1,  2,  6]])
```

而我们的θ

θ初始值为0

In \[7\]:

```
theta=np.zeros((len(x[0]),1))
theta

```

Out\[7\]:

```
array([[0.],
       [0.],
       [0.]])
```

步长为1

In \[8\]:

```
alpha=1

```

# 算法

我用原始的算法吧y(i)θ∙x(i)<0

y(i)θ∙x(i)<0我用矩阵求出所有的y，毕竟这个样本数量少，我可以不考虑资源。

In \[9\]:

```
# 我用y_tmp来表示结果,当y_tmp的某项少于0，那么就表示这个就是误分类啦
y_tmp=x.dot(theta)*y
y_tmp

```

Out\[9\]:

```
array([[-0.],
       [-0.],
       [-0.],
       [ 0.],
       [ 0.],
       [-0.],
       [-0.],
       [-0.],
       [ 0.],
       [ 0.],
       [-0.],
       [ 0.],
       [-0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [-0.]])
```

In \[10\]:

```
error_index=np.where(y_tmp<=0)
# numpy 这点好，大量求，可以说解决了M问题
error_index

```

Out\[10\]:

```
(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
        17], dtype=int64),
 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))
```

In \[12\]:

```
def f1():
    theta=np.zeros((len(x[0]),1))
    y_tmp=x.dot(theta)*y
    error_index=np.where(y_tmp<=0)
    while(len(error_index[0])>0):
        theta=theta+alpha*np.sum(y[error_index[0]]*x[error_index[0]],axis=0).reshape(3,1)
        print(theta.T)
        y_tmp=x.dot(theta)*y
        error_index=np.where(y_tmp<=0)
%time  f1() 

```

```
[[ 0. 53. 33.]]
[[-9. 28. 7.]]
[[-18. 3. -19.]]
[[-9. 81. 40.]]
[[-18. 56. 14.]]
[[-27. 31. -12.]]
[[-32. 13. -23.]]
[[-26. 62. 26.]]
[[-35. 37. 0.]]
[[-44. 12. -26.]]
[[-36. 79. 30.]]
[[-45. 54. 4.]]
[[-54. 29. -22.]]
[[-53. 39. 0.]]
[[-60. 16. -23.]]
[[-53. 71. 28.]]
[[-62. 46. 2.]]
[[-69. 23. -21.]]
[[-65. 53. 16.]]
[[-74. 28. -10.]]
[[-76. 19. -13.]]
[[-73. 39. 16.]]
[[-80. 16. -7.]]
[[-77. 36. 22.]]
[[-85. 12. -3.]]
[[-81. 38. 28.]]
[[-89. 14. 3.]]
[[-90. 8. 1.]]
[[-83. 63. 52.]]
[[-92. 38. 26.]]
[[-99. 15. 3.]]
Wall time: 15 ms

```

# 验证

In \[30\]:

```
# 因为前面加了个运行时间的，所以导致这里不能运行
"""x1=np.arange(x[:,1].min(),x[:,1].max(),0.1)
y1=-(theta[0][0]+theta[1][0]*x1)/theta[2][0]
plt.plot(x1,y1)
for i in x[(np.where(y>0))[0]]:
    plt.plot(i[1],i[2],'x')
for i in x[(np.where(y<0))[0]]:
    plt.plot(i[1],i[2],'o') 
"""

```

Out\[30\]:

```
"x1=np.arange(x[:,1].min(),x[:,1].max(),0.1)\ny1=-(theta[0][0]+theta[1][0]*x1)/theta[2][0]\nplt.plot(x1,y1)\nfor i in x[(np.where(y>0))[0]]:\n    plt.plot(i[1],i[2],'x')\nfor i in x[(np.where(y<0))[0]]:\n    plt.plot(i[1],i[2],'o') \n"
```

# 算法2

In \[14\]:

```
# 初始化数据
#用这种形式更方便一点。
theta=np.zeros((1,len(x[0])))[0] 
alpha=1
y=y.reshape(1,len(y))[0]

```

In \[19\]:

```
# 因为测试运行时间，所以前面的都加到这里去了。
def f2():
    i=0
    theta=np.zeros((1,len(x[0])))[0] 
    alpha=1
    #y=y.reshape(1,len(y))[0]
    y=y=np.array([-1,-1,-1,1,1,-1,-1,-1,1,1,-1,1,-1,1,1,1,1,-1])
    # 迭代每个元素
    while i < len(x):
        #判断是否需要更次参数
        # 判断条件是
        if(y[i]*(theta*x[i]).sum()<=0):
            theta=theta+alpha*y[i]*x[i]
            i=0 # 重新开始啦
            print(theta)
        else:
            i=i+1    
%time f2()

```

```
[-1. -1. -2.]
[0. 9. 6.]
[-1. 8. 4.]
[-2. 7. 2.]
[-3. 6. 0.]
[-4. 5. -2.]
[-5. 3. -4.]
[-6. 0. -5.]
[-5. 10. 3.]
[-6. 9. 1.]
[-7. 8. -1.]
[-8. 6. -3.]
[-9. 3. -4.]
[-8. 13. 4.]
[-9. 12. 2.]
[-10. 11. 0.]
[-11. 10. -2.]
[-12. 8. -4.]
[-13. 5. -5.]
[-12. 15. 3.]
[-13. 14. 1.]
[-14. 13. -1.]
[-15. 11. -3.]
[-16. 9. -5.]
[-17. 6. -6.]
[-16. 16. 2.]
[-17. 15. 0.]
[-18. 13. -2.]
[-19. 11. -4.]
[-20. 8. -5.]
[-19. 14. 4.]
[-20. 13. 2.]
[-21. 11. 0.]
[-22. 9. -2.]
[-23. 6. -3.]
[-22. 12. 6.]
[-23. 11. 4.]
[-24. 9. 2.]
[-25. 6. 1.]
[-26. 2. -3.]
[-25. 12. 5.]
[-26. 10. 3.]
[-27. 8. 1.]
[-28. 5. -5.]
[-27. 15. 3.]
[-28. 13. 1.]
[-29. 11. -1.]
[-30. 8. -2.]
[-29. 14. 7.]
[-30. 12. 5.]
[-31. 10. 3.]
[-32. 7. 2.]
[-33. 4. -4.]
[-32. 14. 4.]
[-33. 12. 2.]
[-34. 9. 1.]
[-35. 5. -3.]
[-34. 15. 5.]
[-35. 13. 3.]
[-36. 10. 2.]
[-37. 7. -4.]
[-36. 13. 5.]
[-37. 11. 3.]
[-38. 8. -3.]
[-37. 14. 6.]
[-38. 12. 4.]
[-39. 9. 3.]
[-40. 6. -3.]
[-39. 16. 5.]
[-40. 14. 3.]
[-41. 11. 2.]
[-42. 8. -4.]
[-41. 14. 5.]
[-42. 11. 4.]
[-43. 8. -2.]
[-42. 14. 7.]
[-43. 12. 5.]
[-44. 9. -1.]
[-45. 3. -3.]
[-44. 13. 5.]
[-45. 10. 4.]
[-46. 7. -2.]
[-45. 13. 7.]
[-46. 10. 6.]
[-47. 7. 0.]
[-46. 13. 9.]
[-47. 10. 8.]
[-48. 7. 2.]
Wall time: 29 ms

```

In \[31\]:

```
# 因为前面加了个运行时间的，导致这里不能运行了
"""x1=np.arange(x[:,1].min(),x[:,1].max(),0.1)
y1=-(theta[0]+theta[1]*x1)/theta[2]
plt.plot(x1,y1)
for i in x[(np.where(y>0))[0]]:
    plt.plot(i[1],i[2],'x')
for i in x[(np.where(y<0))[0]]:
    plt.plot(i[1],i[2],'o') 
"""

```

Out\[31\]:

```
"x1=np.arange(x[:,1].min(),x[:,1].max(),0.1)\ny1=-(theta[0]+theta[1]*x1)/theta[2]\nplt.plot(x1,y1)\nfor i in x[(np.where(y>0))[0]]:\n    plt.plot(i[1],i[2],'x')\nfor i in x[(np.where(y<0))[0]]:\n    plt.plot(i[1],i[2],'o') \n"
```

# 算法3 ，对偶

## 首先是检查数据，因为上边的数据有改变，所以这里查看一下

In \[21\]:

```
x

```

Out\[21\]:

```
array([[ 1,  1,  2],
       [ 1,  2,  2],
       [ 1,  3,  1],
       [ 1, 10,  8],
       [ 1,  6,  9],
       [ 1,  1,  1],
       [ 1,  3,  6],
       [ 1,  4,  4],
       [ 1,  6,  8],
       [ 1,  7,  6],
       [ 1,  3,  2],
       [ 1,  7,  8],
       [ 1,  6,  2],
       [ 1,  9,  6],
       [ 1, 11,  3],
       [ 1, 10,  6],
       [ 1, 12,  5],
       [ 1,  2,  6]])
```

In \[22\]:

```
y

```

Out\[22\]:

```
array([-1, -1, -1,  1,  1, -1, -1, -1,  1,  1, -1,  1, -1,  1,  1,  1,  1,
       -1])
```

## 初始化数据

In \[23\]:

```
a=1 # 步长为1
beta=np.linspace(0,0,len(x))
beta

```

Out\[23\]:

```
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.])
```

## 构造Gram矩阵

In \[24\]:

```
Gram=np.zeros((len(x),len(x)))
Gram

```

Out\[24\]:

```
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.]])
```

In \[25\]:

```
for i in range(len(x)):
    for j in range(len(x)):
        Gram[i][j]=(x[i]*x[j]).sum()
Gram

```

Out\[25\]:

```
array([[  6.,   7.,   6.,  27.,  25.,   4.,  16.,  13.,  23.,  20.,   8.,
         24.,  11.,  22.,  18.,  23.,  23.,  15.],
       [  7.,   9.,   9.,  37.,  31.,   5.,  19.,  17.,  29.,  27.,  11.,
         31.,  17.,  31.,  29.,  33.,  35.,  17.],
       [  6.,   9.,  11.,  39.,  28.,   5.,  16.,  17.,  27.,  28.,  12.,
         30.,  21.,  34.,  37.,  37.,  42.,  13.],
       [ 27.,  37.,  39., 165., 133.,  19.,  79.,  73., 125., 119.,  47.,
        135.,  77., 139., 135., 149., 161.,  69.],
       [ 25.,  31.,  28., 133., 118.,  16.,  73.,  61., 109.,  97.,  37.,
        115.,  55., 109.,  94., 115., 118.,  67.],
       [  4.,   5.,   5.,  19.,  16.,   3.,  10.,   9.,  15.,  14.,   6.,
         16.,   9.,  16.,  15.,  17.,  18.,   9.],
       [ 16.,  19.,  16.,  79.,  73.,  10.,  46.,  37.,  67.,  58.,  22.,
         70.,  31.,  64.,  52.,  67.,  67.,  43.],
       [ 13.,  17.,  17.,  73.,  61.,   9.,  37.,  33.,  57.,  53.,  21.,
         61.,  33.,  61.,  57.,  65.,  69.,  33.],
       [ 23.,  29.,  27., 125., 109.,  15.,  67.,  57., 101.,  91.,  35.,
        107.,  53., 103.,  91., 109., 113.,  61.],
       [ 20.,  27.,  28., 119.,  97.,  14.,  58.,  53.,  91.,  86.,  34.,
         98.,  55., 100.,  96., 107., 115.,  51.],
       [  8.,  11.,  12.,  47.,  37.,   6.,  22.,  21.,  35.,  34.,  14.,
         38.,  23.,  40.,  40.,  43.,  47.,  19.],
       [ 24.,  31.,  30., 135., 115.,  16.,  70.,  61., 107.,  98.,  38.,
        114.,  59., 112., 102., 119., 125.,  63.],
       [ 11.,  17.,  21.,  77.,  55.,   9.,  31.,  33.,  53.,  55.,  23.,
         59.,  41.,  67.,  73.,  73.,  83.,  25.],
       [ 22.,  31.,  34., 139., 109.,  16.,  64.,  61., 103., 100.,  40.,
        112.,  67., 118., 118., 127., 139.,  55.],
       [ 18.,  29.,  37., 135.,  94.,  15.,  52.,  57.,  91.,  96.,  40.,
        102.,  73., 118., 131., 129., 148.,  41.],
       [ 23.,  33.,  37., 149., 115.,  17.,  67.,  65., 109., 107.,  43.,
        119.,  73., 127., 129., 137., 151.,  57.],
       [ 23.,  35.,  42., 161., 118.,  18.,  67.,  69., 113., 115.,  47.,
        125.,  83., 139., 148., 151., 170.,  55.],
       [ 15.,  17.,  13.,  69.,  67.,   9.,  43.,  33.,  61.,  51.,  19.,
         63.,  25.,  55.,  41.,  57.,  55.,  41.]])
```

## 检查是否是误分类的点 y(i)∑j\=1mβjy(j)x(j)∙x(i)<0 y(i)∑j\=1mβjy(j)x(j)∙x(i)<0

 

 

 

In \[26\]:

```
def check(k):
    # 检查第k项是否是误分类的点的
    _tmp=0
    # 对所有的样本
    for m in range(len(x)):
        _tmp=_tmp+beta[m]*y[m]*Gram[m][k]
        pass
    _tmp=y[k]*_tmp
    #判断是否小于等于0
    if _tmp<=0:
        return True
    return False
    pass

```

## 遍历所有元素，寻找误分类的点，并更新

In \[27\]:

```
def f3():
    i=0
    # 迭代每个元素
    while i < len(x):
        #判断是否需要更次参数
        # 判断条件是
        if check(i):
            beta[i]=beta[i]+a
            i=0 # 重新开始啦
            print(beta)
        else:
            i=i+1 
%time f3()

```

```
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[3. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[4. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[5. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[5. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[5. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[5. 1. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[6. 1. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[7. 1. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[7. 2. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[7. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[7. 2. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[8. 2. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[9. 2. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[10. 2. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[10. 3. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[10. 3. 3. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[10. 3. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[11. 3. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[12. 3. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[12. 4. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[12. 5. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[12. 5. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[12. 5. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[13. 5. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[13. 6. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[13. 7. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[13. 7. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[13. 7. 5. 5. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[14. 7. 5. 5. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[14. 8. 5. 5. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[14. 9. 5. 5. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[14. 9. 6. 5. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[14. 9. 6. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 9. 6. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 10. 6. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 10. 7. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 10. 7. 5. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 10. 7. 6. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 11. 7. 6. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 12. 7. 6. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 12. 7. 6. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 12. 7. 7. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 13. 7. 7. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 14. 7. 7. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 14. 8. 7. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 14. 8. 7. 3. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 15. 8. 7. 3. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 16. 8. 7. 3. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 16. 9. 7. 3. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 16. 9. 7. 3. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 16. 9. 8. 3. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 17. 9. 8. 3. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 17. 10. 8. 3. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 17. 10. 8. 3. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 17. 10. 9. 3. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 18. 10. 9. 3. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 18. 11. 9. 3. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 18. 11. 9. 3. 0. 3. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 18. 11. 9. 4. 0. 3. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 19. 11. 9. 4. 0. 3. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 19. 11. 9. 4. 0. 4. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 19. 11. 9. 5. 0. 4. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 20. 11. 9. 5. 0. 4. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 20. 12. 9. 5. 0. 4. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 20. 12. 9. 5. 0. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 20. 12. 10. 5. 0. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 12. 10. 5. 0. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 13. 10. 5. 0. 5. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 13. 10. 5. 0. 6. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 13. 10. 6. 0. 6. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 14. 10. 6. 0. 6. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 14. 10. 6. 0. 7. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 21. 14. 10. 7. 0. 7. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 22. 14. 10. 7. 0. 7. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 22. 14. 10. 7. 0. 8. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[15. 22. 14. 10. 7. 0. 8. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 14. 11. 7. 0. 8. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 15. 11. 7. 0. 8. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 15. 11. 7. 0. 9. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 15. 11. 8. 0. 9. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 16. 11. 8. 0. 9. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 16. 11. 8. 0. 10. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 16. 11. 9. 0. 10. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 17. 11. 9. 0. 10. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
[15. 22. 17. 11. 9. 0. 11. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
Wall time: 58 ms

```

## 计算 Theta θ\=∑j\=1mβjy(j)x(j) θ\=∑j\=1mβjy(j)x(j)

 

 

 

In \[28\]:

```
# 计算theta，
theta=0
for i in range(len(x)):
    theta=theta+(beta[i]*y[i])*x[i]
theta

```

Out\[28\]:

```
array([-48.,   7.,   2.])
```

参考：

- [http://www.cnblogs.com/pinard/p/6042320.html](http://www.cnblogs.com/pinard/p/6042320.html)
