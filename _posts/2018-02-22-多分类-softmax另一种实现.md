---
layout: post
title: "多分类 softmax 2 另一种实现"
date: "2018-02-22"
categories: 
  - "数学"
---

这个是另一个softmax实现，

- 假设概率是 ： $latex \\large p\_j= \\frac{e^{o\_j}}{\\sum\_k e^{o\_k}} $
    - 分母还是求和。
- 损失函数为 ： $latex \\large L = -\\sum\_j y\_j \\log p\_j $
    - 这个损失函数怎么来的？好像不对啊。

我们还是要对损失函数用递归下降法，但看到个$latex p\_j $，先求这个导数吧。

对$latex p\_j 求导。

$$ \\large \\frac{\\partial p\_j}{\\partial o\_i} = \\frac{\\partial \\frac{e^{o\_j}}{\\sum\_k e^{o\_k}}}{\\partial o\_i}\\\\=\\frac{\\frac{\\partial e^{o\_j}}{\\partial e^{o\_i}}\\sum\_k e^{o\_k}-e^{o\_j}e^{o\_i}}{(\\sum\_k e^{o\_k})^2} $$
