---
title: "Sklearn常用特征提取和处理方法"
date: "2020-05-06"
categories: 
  - "scikit-learn"
---

特征分为：连续数值类型和分类类型。其中连续数值类型通常需要做归一化。 分类类型必须因子化

# 将分类变量转换为数值编号，才可以被处理

```
import pandas as pd
import numpy as np
from sklearn import preprocessing

# 用LabelEncoder对不同的犯罪类型编号
leCrime = preprocessing.LabelEncoder()
crime = leCrime.fit_transform(train.Category)
```

# 将分类特征因子化

为什么要因子化： 对于类型因变量。如果仅仅采用数值编码，那最大的问题就是在这种处理方式中，各种类别的特征都被看成是有序的，这显然是非常不符合实际场景的，所以因子化

方法有两种：

## pandas 的 get\_dummies()方法

```
days = pd.get_dummies(train.DayOfWeek)
district = pd.get_dummies(train.PdDistrict)
dummies_Cabin = pd.get_dummies(data_train[‘Cabin’], prefix= ‘Cabin’)
```

## sklearn.preprocessing.OneHotEncoder方法

```
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1],[1, 0, 2]])
print “enc.n_values_ is:”,enc.n_values_

print enc.transform([[0, 1, 1]]).toarray()
```

## 笔记

- 在实际建模前，所有的分类特征，都需要进行因子化
- pd.get\_dummies()是 pandas库提供的快捷方式，但不存储转换信息。OneHotEncoder是比较完备的转换器

# 机器学习中的标准化/归一化

数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。

## 标准化处理的好处：

### 加快梯度下降的求解速度，即提升模型的收敛速度

对于gradient descent算法来说，learning rate的大小对其收敛速度至关重要。如果feature的scale不同，理论上不同的feature就需要设置不同的learning rate，但是gradient descent只有一个learning rate，这就导致不同feature的收敛效果不同，从而影响总体的收敛效果。所以在求解模型之前归一化不同feature的scale，可以有效提高gradient descent的收敛速度。 因此在机器学习中使用梯度下降法求最优解时，归一化也很有必要，否则模型很难收敛甚至有时不能收敛。

### 有可能提高模型的精度

一些分类器需要计算样本之间的距离，如果一个特征的值域范围非常大，那么距离计算就会主要取决于这个特征，有时就会偏离实际情况。 如果feature的scale相差很大，则会出现scale越大的feature，对模型的影响越大。 也需要注意的是，各维分别做归一化会丢失各维方差这一信息，但各维之间的相关系数可以保留

### 不需要对特征进行归一化的情况

基于树的方法不需要进行特征的归一化。例如随机森林，bagging与boosting等方法。如果是基于参数的模型或者基于距离的模型，因为需要对参数或者距离进行计算，都需要进行归一化。

### PCA之前需要做标准化

```
from sklearn.decomposition import PCA
collist=features.columns.tolist()
scaler = preprocessing.StandardScaler()
scaler.fit(features)
features[collist]=scaler.transform(features)

new_PCA=PCA(n_components=60)
new_PCA.fit(features)
print(new_PCA.explained_variance_ratio_)
```

# 标准化处理的常用方法

## 线性标准化（归一化）（min-max normalization）

$$x' = \\frac{x- min(x)}{max(x)-min(x)}$$

这种归一化适合数值比较集中的情况，缺陷就是如果max和min不稳定，很容易使得归一化结果不稳定，使得后续的效果不稳定，实际使用中可以用经验常量来代替max和min。

```
# 使用scikit-learn函数
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
feature_scaled = min_max_scaler.fit_transform(feature)

# 使用numpy自定义函数
def min_max_norm(x):
    x = np.array(x)
    x_norm = (x-np.min(x))/(np.max(x)-np.min(x))
    return x_norm

```

## 标准差标准化（z-score standardization）

$$x^\* = \\frac{x-\\mu}{\\sigma}$$

经过处理的数据符合标准正态分布，均值为0，标准差为1。

```
# 使用scikit-learn函数
from sklearn import preprocessing
standar_scaler = preprocessing.StandardScaler()
feature_scaled = standar_scaler.fit_transform(feature)
# 使用numpy自定义函数
def min_max_norm(x):
    x = np.array(x)
    x_norm = (x-np.mean(x))/np.std(x)
    return x_norm

```

## 非线性归一化

经常用在数据分化较大的场景，有些数值大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括log、指数、反正切等。需要根据数据分布的情况，决定非线性函数的曲线。

log函数：x = lg(x)/lg(max)；反正切函数：x = atan(x)\*2/pi
