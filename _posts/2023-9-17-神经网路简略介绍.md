---
lang: zh
author: Kerwin
layout: post
categories: ["机器学习", "神经网络"]
title:  DNN/RNN/LSTM/GRU的总结
date:   2023-9-14 15:43:00 +0800
excerpt: 神经网路简略介绍
tag:
- pytorch
---

# 神经网络NN
PyTorch的神经网络都在torch.nn包中，$y=Wx+b$,W是权重，b是偏置。
# 神经元
![神经元](/assets/image/NN/343px-Ncell.png) 
   - $a_1~a_n$是输入向量的各个分量
   - $w_1~w_n$ 是权重
   - b 是偏置
   - f 是激活函数
   - t 是输出
# 单层神经网络
有输入和输出的网络，但只有一层，神经元采用阈值函数表示，一组向量的输入，通过训练，阈值函数可以给出0和1的分类输出。
![单程神经网络](/assets/image/NN/331px-SingleLayerNeuralNetwork_english.png)
   - 所有的神经元的输入是一个向量
   - 每个神经元输出一个标量，输出向量的维数等于神经元的数量。
# 两层神经网络
特点是用反向传播，减少运算量，这个也可以多层，但存在梯度消失的问题，原因是这个反向传播，越乘越小。
# 多层神经网络 DNN
PyTorch中有DNN，deep，深度的意思，有Relu激活函数，很好解决了梯度消失的问题。
![深度神经网络](/assets/image/NN/Example_of_a_deep_neural_network.png)  
中间可以有很多层。

# 循环神经网络 RNN
![no img](/assets/image/NN/Recurrent_neural_network_unfold.svg.png)
RNN可以描述动态时间行为，将状态在自身网络中循环传递，因此可以接受时间序列结构输入。

# 长短期记忆 LSTM
![no img](/assets/image/NN/LSTM.png)  
公式如下:  
$$
\begin{aligned}
i^{(t)}=\sigma (W^{(t)}x^{(t)}+U^{(t)}h^{(t-1)}) & (输入阀) & 控制输入传递多少  \\
f^{(t)}=\sigma (W^{(f)}x^{(t)}+U^{(f)}h^{(t-1)}) & (遗忘阀) & 控制上一个的记忆单元传递多少  \\
o^{(t)}=\sigma (W^{(o)}x^{(t)}+U^{(o)}h^{(t-1)}) & (输出) & 控制输出传递多少  \\
\tilde{c}^{(t)}=tanh(W^{(c)}x^{(t)}+U^{(c)}h^{(t-1)}) & 新的记忆单元 & 初步生成记忆单元 \\
c^{(t)}=f^{(t)}\ o \ c^{(t-1)} + i^{(t)}\ o \ \tilde{c}^{t} & 最终的记忆单元 & 传递给下一个神经元 \\
h^{(t)} = o^{(t)}\ o \ \tanh(c^{(t)}) & 输出 & 输出的结果
\end{aligned}
$$

根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate。

# 门控循环单元GRU
![no img](/assets/image/NN/GRU.png)  
比LSTM简单一些

$$
\begin{aligned}
z^{(t)}=\sigma (W^{(z)}x^{(t)}+U^{(z)}h^{(t-1)}) & (更新阀) & 控制上一个的记忆单元传递给新记忆单元多少  \\
r^{(t)}=\sigma (W^{(r)}x^{(t)}+U^{(r)}h^{(t-1)}) & (重置阀) & 控制上一个的隐藏状态和记忆单元传递多少  \\
\tilde{h}^{(t)}=tanh(r^{(t)}\ o \ Uh^{(t-1)} + Wx^{(t)}) & 新的记忆单元 &  是输入和上一个时刻隐藏单元的总结\\
h^{(t)}=(1-z^{(t)})\ o \ \tilde{h}^{(t)} + z^{(t)}\ o \ h^{(t01)} & 隐藏状态 & 是记忆单元跟上一个时刻隐藏单元的总结。
\end{aligned}
$$



# 卷积神经网络 CNN
![卷积图片](/assets/image/NN/convSobel.gif)  

卷积是两个函数的乘积求和，只是其中一个函数翻转后偏移，翻转是以0为中心翻转，这个偏移是将两个函数一致。  
   - 傅里叶变换是将函数分解成一系列的正弦波。
   - 卷积变换时将函数分解成一系列的冲激求和。



# 其他分类
## 感知机
感知机的原理，如果数据可分，感知机尝试寻找一个超平面，能够把所有的二元类别隔离开。  
这里假设有m个样本，每个样本对应n维特征和一个二元类别输出，如下  
$$ (x_1^{(1)},x_2^{(1)},\cdots,x_n^{(1)},y_1),(x_1^{(2)},x_2^{(2)},\cdots,x_n^{(2)},y_2),\cdots,(x_1^{(m)},x_2^{(m)},\cdots,x_n^{(m)},y_m) $$
我们的目标是找到这么一个超平面，即
$$ \theta_0+\theta_1x_1+\cdots+\theta_nx_n=0 $$
让其中一种类别的样品都满足$\theta_0+\theta_1x_1+\cdots+\theta_nx_n >0$,而另一种类别的样品都满足$\theta_0+\theta_1x_1+\cdots+\theta_nx_n<0$ 。  
这样子损失函数可以是，误分类的所有样品到超平面的距离之和最小。这里只是统计误分类的，如果已经正确分类，就不管他。

## 支持向量机
感知机的问题是，尽管没有误分类的了（距离之和都已经是0了），但却存在无数条这种超平面，他们之间也有优劣的，支持向量机寻找那些离超平面很近的点，并且这些点尽可能的原理超平面。


# 引用
   - [DNN、RNN、CNN.…..一文带你读懂这些绕晕人的名词](https://zhuanlan.zhihu.com/p/51241366)
   - [人工神经网络概述](https://zhuanlan.zhihu.com/p/66907125)
   - [人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
   - [长短期记忆](https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6)
   - [Convolutional Neural Networks - Basics](https://mlnotebook.github.io/post/CNN1/)
   - [Deep Learning for NLP1](https://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf)