---
layout: post
title: "【转】卷积神经网络中卷积层和池化层的作用"
date: "2020-04-08"
categories: 
  - "数学"
  - "理论知识"
---

# 前言

假如有一幅1000\*1000的图像，如果把整幅图像作为向量，则向量的长度为1000000（10610^610 6 ）。在假如隐含层神经元的个数和输入一样，也是1000000；那么，输入层到隐含层的参数数据量有 101210^{12}10 12 。所以，我们还得降低维数，同时得以整幅图像为输入（人类实在找不到好的特征了）。 [![no img]](http://127.0.0.1/?attachment_id=4248)

CNN网络一共有5个层级结构：

- 输入层
- 卷积层
- 激活层
- 池化层
- 全连接FC层

# 卷积层

作用是**局部感知**，人的大脑识别图片的过程中，并不是一下子整张图同时识别，而是对于图片中的每一个特征首先局部感知，然后更高层次对局部进行综合操作，从而得到全局信息。

[![no img]](http://127.0.0.1/?attachment_id=4249)

卷积层的计算方法

$$\\begin{aligned} F1 = W\_{11}\\cdot R + W\_{12}\\cdot G + W\_{13}\\cdot B + b\_{1}\\\\F2 = W\_{21}\\cdot R + W\_{22}\\cdot G + W\_{23}\\cdot B + b\_{2}\\\\F3 = W\_{31}\\cdot R + W\_{32}\\cdot G + W\_{33}\\cdot B + b\_{3}\\\\F4 = W\_{41}\\cdot R + W\_{42}\\cdot G + W\_{43}\\cdot B + b\_{4}\\\\\\end{aligned}$$

其中，F表示每一张特征图，W表示卷积核

# 池化层

池化（Pooling）：也称为欠采样或下采样。主要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。主要有：

- Max Pooling：最大池化
- Average Pooling：平均池化

[![no img]](http://127.0.0.1/?attachment_id=4251)

通过池化层，使得原本44的特征图压缩成了22，从而降低了特征维度。

[![no img]](http://127.0.0.1/?attachment_id=4252)

# 输出层（全连接层）

经过前面若干次卷积+激励+池化后，终于来到了输出层，模型会将学到的一个高质量的特征图片全连接层。其实在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元，来解决此问题。还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性。 当来到了全连接层之后，可以理解为一个简单的多分类神经网络（如：BP神经网络），通过softmax函数得到最终的输出。整个模型训练完毕。 两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的：

[![no img]](http://127.0.0.1/?attachment_id=4253)

# 总结

卷积神经网络就是在普通的神经网络前面加了卷积，卷积的输出当作普通神经网络的输入。

 

# 引用

- [卷积神经网络中卷积层和池化层的作用](https://blog.csdn.net/weixin_43843657/article/details/89138646)
