---
layout: post
title: "各种损失函数"
date: "2019-04-10"
categories: 
  - "数学"
---

# L1损失函数 Mean Absolute Error

使用L1损失函数也被叫做最小化绝对误差(Least Absolute Error),就是最小化真实值$latex y\_i$与预测值$latex f(x\_i)$的差值$latex D\_{L1}$的绝对值的和

$$D\_{L1}=\\sum\\limits\_{i=1}^{n} | y\_i-f(x\_i)|$$

# L2损失函数  Mean Square Error / Quadratic Loss

使用L2损失函数也被叫做最小化平方误差(Least Square Error)。就是最小化真实值$latex y\_i$与预测值$latex f(x\_i)$的差值$latex D\_{L2}$的平方的和

$$D\_{L2}=\\sum\\limits\_{i=1}^{n} \\big(y\_i-f\\left(x\_i\\right)\\big)^2$$

# Huber Loss / Smooth Mean Absolute Error 平滑的平均绝对误差

\\\[L\_{\\delta}\\left(y,f(x)\\right)=\\left\\{\\begin{align} &\\frac{1}{2}(y-f(x))^2 &for|y-f(x)|\\leq\\delta\\\\ &\\delta|y-f(x)|-\\frac{1}{2}\\delta^2 &otherwise\\end{align}\\right.\\\]

也就是说，当Huber误差在$latex \[-\\delta,+\\delta\] $中时，等价于MSE，而在$latex \[-\\infty,-\\delta\]和\[\\delta,\\infty\]$时是MAE。

# Log-Cosh Loss

预测误差的双曲余弦的对数。

$$L(y,y^p)=\\sum\_{i=1}^{n}log(cosh(y\_{i}^{p}-y\_{i}))$$

优点：对于较小的x，log(cosh(x))近似等于(x^2)/2，对于较大的x，近似等于abs(x)-log(2)。这意味着‘logcosh’基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，Log-cosh二阶处处可微。

为什么需要二阶导数？许多机器学习模型如XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如XGBoost这类机器学习框架，损失函数的二阶可微是很有必要的。

但Log-cosh损失也并非完美，其仍存在某些问题。比如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。

# 分位数损失

在大多数现实世界预测问题中，我们通常希望了解预测中的不确定性。清楚预测的范围而非仅是估计点，对许多商业问题的决策很有帮助。

当我们更关注区间预测而不仅是点预测时，分位数损失函数就很有用。使用最小二乘回归进行区间预测，基于的假设是残差（y-y\_hat）是独立变量，且方差保持不变。

一旦违背了这条假设，那么线性回归模型就不成立。但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。

下面让我们看一个实际的例子，以便更好地理解基于分位数损失的回归是如何对异方差数据起作用的。

## 分位数回归与最小二乘回归

[![]](http://127.0.0.1/?attachment_id=2345)

左：b/wX1和Y为线性关系。具有恒定的残差方差。右：b/wX2和Y为线性关系，但Y的方差随着X2增加。（异方差）

[![]](http://127.0.0.1/?attachment_id=2346)

橙线表示两种情况下OLS的估值

[![]](http://127.0.0.1/?attachment_id=2347)

分位数回归。虚线表示基于0.05和0.95分位数损失函数的回归

附上图中所示分位数回归的代码：

https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09\_Quantile\_Regression.ipynb

## 理解分位数损失函数

如何选取合适的分位值取决于我们对正误差和反误差的重视程度。损失函数通过分位值（γ）对高估和低估给予不同的惩罚。例如，当分位数损失函数γ=0.25时，对高估的惩罚更大，使得预测值略低于中值。

$$L\_{\\gamma}(y,y^p)=\\sum\_{i:yi<y\_{i}^{p}}(1-\\gamma)|y\_{i}-y\_{i}^{p}|+\\sum\_{i:yi\\ge y\_{i}^{p}}\\gamma|y\_{i}-y\_{i}^{p}|$$

γ是所需的分位数，其值介于0和1之间。[![]](http://127.0.0.1/?attachment_id=2349)

 

# 对比研究

为了证明上述所有损失函数的特点，让我们来一起看一个对比研究。首先，我们建立了一个从sinc（x）函数中采样得到的数据集，并引入了两项人为噪声：高斯噪声分量ε〜N（0，σ2）和脉冲噪声分量ξ〜Bern（p）。

加入脉冲噪声是为了说明模型的鲁棒效果。以下是使用不同损失函数拟合GBM回归器的结果。

[![]](http://127.0.0.1/wp-content/uploads/2019/04/20180918144853440.png)

连续损失函数：（A）MSE损失函数；（B）MAE损失函数；（C）Huber损失函数；（D）分位数损失函数。将一个平滑的GBM拟合成有噪声的sinc（x）数据的示例：（E）原始sinc（x）函数；（F）具有MSE和MAE损失的平滑GBM；（G）具有Huber损失的平滑GBM，且δ={4,2,1}；（H）具有分位数损失的平滑的GBM，且α={0.5,0.1,0.9}。

仿真对比的一些观察结果：

MAE损失模型的预测结果受脉冲噪声的影响较小，而MSE损失函数的预测结果受此影响略有偏移。

Huber损失模型预测结果对所选超参数不敏感。

分位数损失模型在合适的置信水平下能给出很好的估计。

最后，让我们将所有损失函数都放进一张图，我们就得到了下面这张漂亮的图片！它们的区别是不是一目了然了呢~

[![]](http://127.0.0.1/wp-content/uploads/2019/04/20180918144909629.png)
