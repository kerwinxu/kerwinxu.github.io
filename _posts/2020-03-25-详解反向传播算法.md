---
layout: post
title: "详解反向传播算法"
date: "2020-03-25"
categories: 
  - "理论知识"
---

# 求导方法

## 用计算图来解释几种求导方法

式子 $latex e = (a+b)\*(b+1)$ 可以用如下计算图表达：

[![no img]](http://127.0.0.1/?attachment_id=3253)

令a=2,b=1则有：

[![no img]](http://127.0.0.1/?attachment_id=3254)

 

如何在计算图上表达“求导”呢？ 导数的含义是 因变量随自变量的变化率，例如 $latex \\frac{\\partial y}{\\partial x} = 3 $ 表示当x变化1个单位，y 会变化3个单位，微积分的加法法则和乘法法则，我们在计算图的边上表示导数或偏导数

 

[![no img]](http://127.0.0.1/?attachment_id=3256)

那么 $latex \\frac{\\partial e}{\\partial b} $ 如何求呢，我们要所有的路径都要考虑，$latex \\frac{\\partial e}{\\partial b} =  \\frac{\\partial c}{\\partial b} \* \\frac{\\partial e}{\\partial c} + \\frac{\\partial d}{\\partial b} \* \\frac{\\partial e}{\\partial d} = 1\*2+1\*3 = 5$

[![no img]](http://127.0.0.1/?attachment_id=3258)

 

所以上面的求导方法总结为一句话就是： 路径上所有边相乘，所有路径相加。不过这里需要补充一条很有用的合并策略：

面的计算图若要计算求导就会有9条路径

[![no img]](http://127.0.0.1/?attachment_id=3260)

[![no img]](http://127.0.0.1/?attachment_id=3259)

如果计算图再复杂一些，层数再多一些，路径数量就会呈指数爆炸性增长。但是如果采用合并策略[![no img]](http://127.0.0.1/?attachment_id=3261)，就不会出现这种问题。这种策略不是 对每一条路径都求和，而是 “合并同类路径”，“分阶段求解”。先求X对Y的总影响，[![no img]](http://127.0.0.1/?attachment_id=3262)，

再求Y对Z的总影响 [![no img]](http://127.0.0.1/?attachment_id=3263) 最后综合在一起。

两种求导模式

- 前向求导模式，forward-mode differentiation
    - 前向求导模式追踪一个输入如何影响每一个节点（对每一个节点进行 $latex \\frac{\\partial}{\\partial X}$操作
    - [![no img]](http://127.0.0.1/?attachment_id=3264)
- 反向求导模式，reverse-mode differentiation
    - 反向求导模式追踪每一个节点如何影响一个输出（对每一个节点进行 $latex \\frac{\\partial Z}{\\partial}$操作）
    - [![no img]](http://127.0.0.1/?attachment_id=3265)

## 反向求导的重要性

让我们再次考虑前面的例子：

[![no img]](http://127.0.0.1/?attachment_id=3266)

如果用前向求导模式：关于b向前求导一次

[![no img]](http://127.0.0.1/?attachment_id=3267)

如果用反向求导模式：向后求导

[![no img]](http://127.0.0.1/?attachment_id=3268)

前向求导只得到了关于b的求导，如果还需要求a的偏导，那么还需要再计算一次，而反向求导，一次运算就得到了对输入a，b的偏导。

## 神经网络结构图：

# [![no img]](http://127.0.0.1/?attachment_id=3274)

其中C是损失函数，例如C可以取：

[![no img]](http://127.0.0.1/?attachment_id=3275)

梯度下降（SGD）进行学习时，核心问题是求解损失函数C关于所有网络参数![[公式]](https://www.zhihu.com/equation?tex=w_%7Bjk%7D%2Cb_j)的偏导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w_%7Bjk%7D%7D+%2C%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_j%7D++)。 根据[详解反向传播算法(上)](https://zhuanlan.zhihu.com/p/25081671) 我们已经知道用反向传播算法可以“一次反向计算”得到损失函数C关于网络中所有参数的偏导数。模仿[详解反向传播算法(上)](https://zhuanlan.zhihu.com/p/25081671) 的推理过程，我们首先画出上面网络图的详细计算图：再看看具体怎么样反向传播求偏导数。

## 神经网络计算图

对应计算图如下：（只展开了最后两层的计算图）：![no img](https://pic1.zhimg.com/80/v2-37c5e8de23a7425c85366980df9f5ca0_720w.png)绿色代表权重参数![[公式]](https://www.zhihu.com/equation?tex=w_%7Bjk%7D),橙色代表基底参数![[公式]](https://www.zhihu.com/equation?tex=b_j)。可见虽然网络图上只是简单几条线，计算图还是蛮复杂的。

现在我们在计算图箭头上标出对应的偏导数（只标出了一部分）。![no img](https://pic2.zhimg.com/80/v2-9a4aff21fc12d343cc3a4f1c663e8c91_720w.png)

## 反向传播四公式

 

上面计算图上每一个节点关于前一个节点的偏导数都可以求得，根据求导的链式法则，想要求损失函数C关于某一节点的偏导数，只需要“把该节点每条反向路径上的偏导数做乘积，再求和”即可。（![[公式]](https://www.zhihu.com/equation?tex=w_%7Bjk%7D%2Cb_j)分别对应绿色和橙色的节点）

现在我们已经可以在计算图上求得损失函数C关于模型参数的偏导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w_%7Bjk%7D%7D+%2C%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_j%7D++)。但是还不够优雅，反向传播算法要优雅的很多，它通过定义一个损失（![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5El)），先逐层向后传播得到每一层节点的损失（![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5El)），再通过每一个节点的损失（![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5El)）来求解该节点的![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w_%7Bjk%7D%7D+%2C%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_j%7D++)。

 

首先记损失函数C关于![[公式]](https://www.zhihu.com/equation?tex=l)层的第j个元素的偏导为：![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5El+%5Cequiv+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5El%7D)，

**最后一层**

对于最后一层（L层）的元素j会有：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5EL+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_j%5EL%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_j%5EL%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+a_j%5EL%7D%7B%5Cpartial+z_j%5EL%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_j%5EL%7D+%5Ccdot+%5Csigma%5E%7B%27%7D%28z_j%5EL%29)

向量化为:

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm+%5Cdelta%5EL+%3D+%5Cbegin%7Bpmatrix%7D%0A+%5Cdelta_1%5EL+%5C%5C%0A%5Cvdots+%5C%5C%0A+%5Cdelta_j%5EL+%5C%5C+%0A++%5Cvdots%5C%5C+%0A+%5Cdelta_n%5EL%0A+%5Cend%7Bpmatrix%7D%0A%3D%0A+%5Cbegin%7Bpmatrix%7D%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_1%5EL%7D+%5Ccdot+%5Csigma%5E%7B%27%7D%28z_1%5EL%29+%5C%5C%0A%5Cvdots+%5C%5C%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_j%5EL%7D+%5Ccdot+%5Csigma%5E%7B%27%7D%28z_j%5EL%29+%5C%5C+%0A++%5Cvdots%5C%5C+%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_n%5EL%7D+%5Ccdot+%5Csigma%5E%7B%27%7D%28z_n%5EL%29%0A+%5Cend%7Bpmatrix%7D%0A%3D%0A+%5Cbegin%7Bpmatrix%7D%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_1%5EL%7D+%5C%5C%0A%5Cvdots+%5C%5C%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_j%5EL%7D+%5C%5C+%0A++%5Cvdots%5C%5C+%0A+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a_n%5EL%7D%0A+%5Cend%7Bpmatrix%7D%0A%5Codot+%0A+%5Cbegin%7Bpmatrix%7D%0A+%5Csigma%5E%7B%27%7D%28z_1%5EL%29+%5C%5C%0A%5Cvdots+%5C%5C%0A+%5Csigma%5E%7B%27%7D%28z_j%5EL%29+%5C%5C+%0A++%5Cvdots%5C%5C+%0A+%5Csigma%5E%7B%27%7D%28z_n%5EL%29%0A+%5Cend%7Bpmatrix%7D%0A+%3D++%5Cbm+%5Cnabla_aC+%5Codot++%5Csigma%5E%7B%27%7D%28%5Cbm+z%5EL%29) **(BP1)**

其中![[公式]](https://www.zhihu.com/equation?tex=%5Codot)的操作是把两个向量对应元素相乘组成新的元素。

**后一层传播到前一层**

由前面计算图中L和L-1层所标注的偏导数，可得到倒数第一层（L-1）元素j的损失为：（请仔细对照前面的计算图）![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_j%5E%7BL-1%7D+%3D+%28%5Csum_%7Bj%3D1%7D%5En%7B%5Cfrac%7B%5Cpartial+z_j%5EL%7D%7B%5Cpartial+a_%7Bk%7D%5E%7BL-1%7D%7D++%5Cdelta_j%5EL+%7D%29+%5Ccdot+%5Csigma_%7B%27%7D%28z_j%5E%7BL-1%7D%29+%3D+%28%5Csum_%7Bj%3D1%7D%5En%7Bw_%7Bjk%7D%5EL+%5Cdelta_j%5EL+%7D+%29+%5Ccdot++%5Csigma_%7B%27%7D%28z_j%5E%7BL-1%7D%29++%3D%0A%5Cbegin%7Bpmatrix%7D%0A+w_%7B1k%7D%5EL+%5Ccdots+w_%7Bjk%7D%5EL+%5Ccdots++w_%7Bnk%7D%5EL%5C%5C+%0A+%5Cend%7Bpmatrix%7D+%0A%5Cbegin%7Bpmatrix%7D%0A+%5Cdelta_1%5EL+%5C%5C+%0A+%5Cvdots+%5C%5C+%0A+%5Cdelta_j%5EL%5C%5C%0A%5Cvdots%5C%5C%0A%5Cdelta_n%5EL%0A+%5Cend%7Bpmatrix%7D%0A%5Ccdot+%5Csigma%5E%7B%27%7D%28z_j%5E%7BL-1%7D%29)

向量化：![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7BL-1%7D+%3D+%28%28w%5E%7BL%7D%29%5ET%5Cdelta%5E%7BL%7D+%5Codot+%5Csigma%5E%7B%27%7D%28z%5E%7BL-1%7D%29+%29)

这启发我们后一层（![[公式]](https://www.zhihu.com/equation?tex=l%2B1)层）的损失![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7Bl%2B1%7D) 如何传播到前一层（![[公式]](https://www.zhihu.com/equation?tex=l)层）得到![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5El)。(只需要把L用![[公式]](https://www.zhihu.com/equation?tex=l%2B1)替换，![[公式]](https://www.zhihu.com/equation?tex=L-1)用![[公式]](https://www.zhihu.com/equation?tex=l)替换)就得到了逐层传播损失的公式：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm+%5Cdelta%5E%7Bl%7D+%3D+%28%28%5Cbm+w%5E%7Bl%2B1%7D%29%5ET+%5Cbm+%5Cdelta%5E%7Bl%2B1%7D+%5Codot+%5Csigma%5E%7B%27%7D%28%5Cbm+z%5E%7Bl%7D%29+%29)**(BP2)**

 

**关于![[公式]](https://www.zhihu.com/equation?tex=b_j%5El)的偏导数**

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_j%5El%7D+%3D%0A%5Cfrac%7B+%5Cpartial+C%7D%7B+%5Cpartial+z_j%5El%7D+%5Cfrac%7B%5Cpartial+z_j%5El%7D%7B%5Cpartial+b_j%5El%7D+%3D+%5Cdelta_j%5El+%5Ccdot+1+%3D+%5Cdelta_j%5El)**(BP3)**

向量化：![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%5El%7D+%3D%5Cbm+%5Cdelta%5El)

**关于![[公式]](https://www.zhihu.com/equation?tex=w_%7Bjk%7D%5El)的偏导数**

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w_%7Bjk%7D%5El%7D+%3D%0A%5Cfrac%7B+%5Cpartial+C%7D%7B+%5Cpartial+z_j%5El%7D+%5Cfrac%7B%5Cpartial+z_j%5El%7D%7B%5Cpartial+w_%7Bjk%7D%5El%7D+%3D+%5Cdelta_j%5El+%5Ccdot+a_k%5E%7Bl-1%7D+)**（BP4）**

向量化：![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w_%7Bj%5Ccdot+%7D%5El%7D+%3D%0A%5Cbegin%7Bpmatrix%7D%0A%5Cdelta_j%5El++a_1%5E%7Bl-1%7D++%5C%5C+%0A+%5Cvdots+%5C%5C+%0A+%5Cdelta_j%5El++a_k%5E%7Bl-1%7D+%5C%5C%0A%5Cvdots%5C%5C%0A%5Cdelta_j%5El++a_n%5E%7Bl-1%7D+%0A+%5Cend%7Bpmatrix%7D%0A%3D%0A%5Cdelta_j%5El+%5Ccdot%0A%5Cbegin%7Bpmatrix%7D%0A++a_1%5E%7Bl-1%7D++%5C%5C+%0A+%5Cvdots+%5C%5C+%0A+++a_k%5E%7Bl-1%7D+%5C%5C%0A%5Cvdots%5C%5C%0A++a_n%5E%7Bl-1%7D+%0A+%5Cend%7Bpmatrix%7D%0A%3D+%5Cdelta_j%5El+%5Ccdot+%5Cbm+a%5E%7Bl-1%7D+)![[公式]](https://www.zhihu.com/equation?tex=%5CRightarrow+)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%5El%7D+%3D+%0A%5Cbegin%7Bpmatrix%7D%0A%5Cdelta%5El_1+%5Ccdot+%5Cbm+a%5E%7Bl-1%7D+%5C%5C%0A%5Cvdots+%5C%5C%0A%5Cdelta%5El_j+%5Ccdot+%5Cbm+a%5E%7Bl-1%7D%5C%5C%0A%5Cvdots%5C%5C%0A%5Cdelta%5El_n+%5Ccdot+%5Cbm+a%5E%7Bl-1%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%0A%3D+%0A%5Cbegin%7Bpmatrix%7D%0A%5Cdelta%5El_1+++%5C%5C%0A%5Cvdots+%5C%5C%0A%5Cdelta%5El_j+%5C%5C%0A%5Cvdots%5C%5C%0A%5Cdelta%5El_n+%5C%5C%0A%5Cend%7Bpmatrix%7D%0A%5Ccdot+%0A%5Cbegin%7Bpmatrix%7D%0A+a%5E%7Bl-1%7D_1++%5Ccdots+a%5E%7Bl-1%7D_k+%5Ccdots+a%5E%7Bl-1%7D_n++%0A%5Cend%7Bpmatrix%7D%0A%3D+%5Cbm+%5Cdelta%5El+%5Ccdot+%28%5Cbm+a%5E%7Bl-1%7D%29%5ET)

至此就得到了反向传播的4个公式：

 ![no img](https://pic4.zhimg.com/80/v2-72ad4203fb6da5ad4bf33e36c433c7db_720w.png)图片来自：[Neural networks and deep learning](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html) 反向传播算法流程：![nimg](https://pic3.zhimg.com/80/v2-1534d5eb4821acbe9eda5ff01beae5ee_720w.png)流程图来自：[Neural networks and deep learning](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html)

本文主要参考 [Neural networks and deep learning](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html)，原作者写的也不错，不过个人觉得如果按照计算图会更加直观，基本不需要数学推导过程，用肉眼看图就可以理解反向传播的四个公式。当然前提是计算图要画的清晰明白。花了半天时间来写这篇文章，其中画图花费了80%的时间，尤其是计算图改了N次，仍然可能存在错误，欢迎指正~

点赞（分享）就是对文章作者的最大鼓励~

 

\------下面只是备份下用过的公式，以备后面修改使用 ------------

![[公式]](https://www.zhihu.com/equation?tex=a_j%5EL%3D%5Csigma%28z_j%5EL%29++)![[公式]](https://www.zhihu.com/equation?tex=a_2%5EL%3D%5Csigma%28z_2%5EL%29++)![[公式]](https://www.zhihu.com/equation?tex=z_j%5EL%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7B%28w_%7Bjk%7D%5EL+%5Ccdot+a_%7Bk%7D%5E%7BL-1%7D%29+%7D+%2Bb_%7Bj%7D%5EL)![[公式]](https://www.zhihu.com/equation?tex=z_1%5EL%3D%5Csum_%7Bk%3D1%7D%5E%7B4%7D%7B%28w_%7B1k%7D+%5Ccdot+a_k%5E%7BL-1%7D%29+%7D+%2Bb_%7B1%7D)![[公式]](https://www.zhihu.com/equation?tex=z_2%5EL%3D%5Csum_%7Bk%3D1%7D%5E%7B4%7D%7B%28w_%7B2k%7D+%5Ccdot+a_k%5E%7BL-1%7D%29+%7D+%2Bb_%7B2%7D)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+a_j%5EL%7D%7B%5Cpartial+z_j%5EL%7D++%3D+%5Csigma%5E%7B%27%7D%28z_j%5EL%29)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+z_j%5EL%7D%7B%5Cpartial+b_j%5EL%7D+%3D+1)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+z_j%5EL%7D%7B%5Cpartial+w_%7Bjk%7D%5EL%7D++%3Da_k%5E%7BL-1%7D)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+z_2%5EL%7D%7B%5Cpartial+a_%7Bk%7D%5E%7BL-1%7D%7D++%3Dw_%7B2k%7D%5EL) ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+a_k%5E%7BL-1%7D%7D%7B%5Cpartial+z_k%5E%7BL-1%7D%7D++%3D+%5Csigma%5E%7B%27%7D%28z_k%5E%7BL-1%7D%29)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+z_k%5E%7BL-1%7D%7D%7B%5Cpartial+b_k%5E%7BL-1%7D%7D+%3D+1)![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+z_k%5E%7BL-1%7D%7D%7B%5Cpartial+w_%7Bkm%7D%5E%7BL-1%7D%7D++%3Da_m%5E%7BL-2%7D)

\------------------------ 备份 end ----------------------------------------------

# 总结

在神经网络中，假设每层的函数是

- $latex z = w\\cdot a + b$
- 激活函数： $latex a = f(z)$

所谓的正向传播，就是输入x值，然后根据假定的w,b值计算出y值。

所谓的反向传播，就是根据计算出的y值，跟真实的y值的损失来不断的往前推导a值，然后推导出w,b值。

# 引用

- [详解反向传播算法(上)](https://zhuanlan.zhihu.com/p/25081671)
