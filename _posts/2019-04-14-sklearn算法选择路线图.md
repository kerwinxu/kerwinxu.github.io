---
layout: post
title: "SkLearn算法选择路线图"
date: "2019-04-14"
categories: ["计算机语言", "Python"]

---

[![no img]](http://127.0.0.1/wp-content/uploads/2020/11/v2-6c2e844aaa6ae4fdd3924bb496c2f421_r.jpg)

[![no img]](http://127.0.0.1/wp-content/uploads/2019/04/20160930051801241.png)

英文原版链接：[http://scikit-learn.org/stable/tutorial/machine\_learning\_map/](http://scikit-learn.org/stable/tutorial/machine_learning_map/)

# 监督学习

## 广义线性模型

sk-learn中约定的线性模型为 $ \\hat{y}(w,x) = w\_{0}+w\_{1}x\_{1}+ \\cdots +w\_{p}x\_{p} $

- 定义 $ w = (w\_1,\\cdots,w\_p)$ 为 **coef\_**  ，这个就是系数啦
- 定义 $ w\_0 $ 为intercept\_  ，这个就是b啦。

如下

<table style="height: 672px; width: 100%; border-collapse: collapse;" border="1"><tbody><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">类</td><td style="width: 22.6665%; height: 24px;">名称</td><td style="width: 36.4848%; height: 24px;">损失函数</td></tr><tr style="height: 24px;"><td style="text-align: center; width: 78.3029%; height: 24px;" colspan="3">sklearn.linear_model 包</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">LinearRegression</td><td style="width: 22.6665%; height: 24px;">最小二乘法</td><td style="width: 36.4848%; height: 24px;">$ {\underset{w}{\min} || X_w - y ||_2}^2$</td></tr><tr style="height: 48px;"><td style="width: 19.1516%; height: 48px;">Ridg</td><td style="width: 22.6665%; height: 48px;">岭回归</td><td style="width: 36.4848%; height: 48px;">$ {\underset{w}{\min} || X_w - y ||_2}^2+{a||w||_2}^2$</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">RidgeCV</td><td style="width: 22.6665%; height: 24px;">广义交叉验证岭回归</td><td style="width: 36.4848%; height: 24px;">&nbsp;</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">Lasso</td><td style="width: 22.6665%; height: 24px;">Lasso回归（L1正则）</td><td style="width: 36.4848%; height: 24px;">$ {\underset{w}{\min}\frac{1}{2n_{samples}} || X_w - y ||_2}^2+{a||w||_1}$</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">LassoCV</td><td style="width: 22.6665%; height: 48px;" colspan="2" rowspan="2">对超参数α使用了交叉验证，来帮忙我们选择一个合适的α</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">LassoLarsCV</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">LassoLarsIC</td><td style="width: 22.6665%; height: 24px;">自动找到超参数</td><td style="width: 36.4848%; height: 24px;">用LassoLarsIC类我们可以一轮找到超参数α</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">ElasticNet</td><td style="width: 59.1513%; height: 48px;" colspan="2" rowspan="2">$ J(\mathbf\theta) = \frac{1}{2m}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y}) + \alpha\rho||\theta||_1 + \frac{\alpha(1-\rho)}{2}||\theta||_2^2$</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">ElasticNetCV</td></tr><tr style="height: 96px;"><td style="width: 19.1516%; height: 96px;">OrthogonalMatchingPursuit</td><td style="width: 22.6665%; height: 96px;">限制非0的最大个数</td><td style="width: 36.4848%; height: 96px;">$ J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})$</td></tr><tr style="height: 72px;"><td style="width: 19.1516%; height: 72px;">MultiTaskLasso</td><td style="width: 59.1513%; height: 96px;" colspan="2" rowspan="2">$ J(\mathbf{W}) = \frac{1}{2m}\mathbf{(||XW-Y||)_{Fro}^2} + \alpha||\mathbf{W}||_{21}$</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">MultiTaskLassoCV</td></tr><tr style="height: 120px;"><td style="width: 19.1516%; height: 120px;">MultiTaskElasticNet</td><td style="width: 59.1513%; height: 144px;" colspan="2" rowspan="2">$ J(\mathbf{W}) = \frac{1}{2m}\mathbf{(||XW-Y||)_{Fro}^2} + \alpha\rho||\mathbf{W}||_{21} + \frac{\alpha(1-\rho)}{2}\mathbf{(||W||)_{Fro}^2}$</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">MultiTaskElasticNetCV</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">BayesianRidge</td><td style="width: 22.6665%; height: 24px;">贝叶斯回归模型</td><td style="width: 36.4848%; height: 24px;">&nbsp;</td></tr><tr style="height: 24px;"><td style="width: 19.1516%; height: 24px;">ARDRegression</td><td style="width: 22.6665%; height: 24px;">贝叶斯回归模型</td><td style="width: 36.4848%; height: 24px;">&nbsp;</td></tr></tbody></table>

# 引用

- [https://sklearn.apachecn.org/docs/0.21.3/](https://sklearn.apachecn.org/docs/0.21.3/) sk-learn中文文档
