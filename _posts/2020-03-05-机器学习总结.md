---
title: "机器学习总结"
date: "2020-03-05"
categories: 
  - "scikit-learn"
---

# 机器学习开发流程

1. 获取数据
2. 数据处理
3. 特征工程
4. 机器学习算法训练 —— 模型
5. 模型评估
6. 应用

# 获取数据

- 可用数据集
    - sklearn
    - UCI
        - 网址：
            - http://archive.ics.uci.edu/ml/index.php
        - 特点
            - 收录了400+个数据集
            - 覆盖科学、生活、经济等领域
            - 数据集几十万
    - Kaggle
        - 网址：
            - https://www.kaggle.com/datasets
        - 特点
            - 大数据竞赛平台
            - 80万科学家
            - 真实数据
            - 数据量巨大
- 数据集的划分
    - 训练数据集（70%~80%）：用于训练，构建模型
    - 测试数据集（20%~80%）：在模型检验时使用，用于评估模型是否有效

# 特征工程

- 内容包括
    - 特征抽取/特征提取
        - 提取API： sklearn.feature\_extraction
        - 字典特征提取
            - 类别 -> one-hot编码：sklearn.feature\_extraction.DictVectorizer(sparse = True)
            - 父类：transfer类
            - 返回值：sparse 矩阵
                - sparse（稀疏）：将非零值按位置表示出来；节省内存，提高加载效率
            - 应用场景
            - pclass,sex 等，数据集中类别比较多
                - 1、将数据集的特征 ——》字典类型
                - 2、DictVectorizer 转换 本身拿
            - 到的数据就是字典类型
        - 文本特征提取
            - 特征：特征词（单词）
            - 提取方法：
                - 1、CountVectorizer ：统计每个单词特征词出现的个数
                - 2、TfidfVectorizer ：统计每个单词特征的重要程度
                    - TF：词频
                    - IDF：逆向文档频率
    - 特征预处理
        - API
            - sklearn.preprocessing
        - 内容：数值型数据的无量纲化
            - 归一化
                - sklearn.preprocessing.MinMaxSxaler(feature\_range=(0,1)…)
                - 存在问题：若存在异常值，则对最大值和最小值的影响较大。
                - 缺点：鲁棒性较差
                - 应用场景：只适合传统精确小数据场景
            - 标准化
                - sklearn.preprocessing.StandardScaler()：处理后，所有数据都聚集在均值为0，标准差
                - 为1的附近
                - 公式：(x - means) / std
                - 标准差：集中程度
                - 优点：一定量数据集下,若存在少量的异常值，对平均值和标准差的影响不大。
                - 应用场景：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景
    - 特征降维 - 降低维度
        - 概述
            - 降维是指在某些限定条件下，降低随机变量（特征）个数。得到一组“不相关”主变量的过程
        - 内容
            - 特征选择
                - 定义：数据中包含冗余或相关变量（特征），旨在从原有特征中找出主要特征
                - 方法
                    - Filter（过滤式）：主要探究特征本身特点、特征与特征和目标值之间的关联
                        - 方差选择法：低方差特征过滤
                            - API：sklearn.feature\_selection.VarianceThreshold(theshold = 0.0) ：删除所有低方差特征
                        - 相关系数法：特征与特征之间的相关程度
                            - 皮尔逊相关系数：反应变量之间相关关系密切程度的统计指标
                                - API： from scipy.stats import pearsonr pearsonr(特征1向量，特征2向量) 返回值：（相关系数，p值）
                            - 也可以通过画图来观察结果
                            - 特点：相关系数的值介于 -1~1 之间
                                - 当 r >0,表示两变量正相关，r < 0 ,表示两向量负相关
                                - 当 | r | = 1时，表示两向量完全相关，当| r | = 0时，表示两向量无相关关系
                                - 当 0<|r|<1时，表示两向量存在一定的相关程度；且 | r | 越接近1，两变量间线性关系越密切；| r | 越接近0，两变量间的线性相关性越弱
                                - 一般可按三级划分：| r | <0.4为低度相关；0.4 < | r | <0.7为显著性相关；| r |>0.7为高度线性相关
                            - 结果：特征与特征之间的相关性很高
                                - 1、选取其中一个
                                - 2、加权求和
                                - 3、主成分分析
                    - Embedded(嵌入式)： 算法自动选择特征
                        - 决策树：信息熵、信息增益
                        - 正则化：L1、L2
                        - 深度学习：卷积等
            - 主成分分析（PCA）
                - 定义：高维数据转换为低维数据的过程，在此过程中可能会舍弃原有数据，创造新的变量
                - 作用：数据维度压缩，尽可能降低原数据的维度(复杂度)，损失少量信息
                - 应用场景：回归分析或者聚类分析当中
                - API：sklearn.decomposition.PCA(n\_components = None)
                    - n\_components 小数：表示保留百分之多少的信息 整数：表示减少到多少特征
