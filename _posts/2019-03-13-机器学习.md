---
title: "机器学习"
date: "2019-03-13"
categories: 
  - "数学"
---

# 回归

## 线性回归

- 公式
    - $latex y=h(x)=\\theta\_0+\\theta\_{1}x\_{2}+\\theta\_{2}x\_{2}=\\theta^{T}X$
- 损失函数
    - 公式
        - $latex J(\\theta)=\\frac{1}{2}\\sum\_{i=1}^{m}\\left(h\_{\\theta}(x^{(i)})-y^{(i)}\\right)^2$
        - 求 $latex \\underset{\\theta}{min}J\_{\\theta}$
    - 本质
        - 估计值$latex h\_{\\theta}(x^{(i)})$与真实值$latex y^{(i)}$差值的平方和。
- 计算方法
    - [梯度下降](http://127.0.0.1/?p=824)
    - 最小二乘法

## [Logistic回归](http://127.0.0.1/?p=859)

两种理解

- 李宏毅的《机器学习》视频中的
    - 由如下两个公式
        - $latex /sigma(z) = \\frac{1}{1+e^{-z}}$
        - $latex f\_{w,b}(x) = \\sigma\\left(\\sum\_limits\_i w\_i x\_i +b\\right)$
- 另外一个教程上是这样的
    - 前提：
        - 可以将分类任务理解成估计事件发生的概率p（这个是我们要求的）
    - 思想
        - $latex  odds= \\frac {p}{1-p} $ ，优势比替代概率 , 计算出来的结果区间是 $latex \[ 0 \\to \\infty \] $
        - $latex \\eta =log(odds)=log \\frac {p}{1-p} = logit(p) $ 优势比的对数，这时候计算出来的结果区间就是 $latex\[-\\infty \\to \\infty \]$
        - sigmoid函数
            - $latex g(x)=\\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}} $
                - $latex P(y=1 | X) = \\frac {e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}}$
                - $latex P(y=0 | X) = \\frac {1}{1+e^{\\theta^{T} \\cdot x}}$
                - P(y=1 | X) 表示给定参数X，求y=1时的概率。
                - $latex \\theta$是未知参数
        - $latex \\prod^m\_{i=1}\\underbrace{P(y^{(i)}=1|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=1} \\cdot \\underbrace{P(y^{(i)}=0|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=0}=\\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} $
            - 所有样品的分布概率的连乘积。
            - 连乘积最大的时候，这个参数就是最优的。
        - 连乘积难求，两边求对数，连乘积转变成求和的形式，次幂形式转变成乘法运算。
        - 梯度下降法
        - 总结： $latex \\begin{align\*} & \\frac{p}{1-p}=f(x)=\\theta^T x & 优势比 \\\\ & \\log \\frac{p}{1-p} = f\_\\theta(x)=\\theta^T x & 对数优势比 \\\\ & \\prod^m\_{i=1}\\underbrace{P(y^{(i)}=1|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=1} \\cdot \\underbrace{P(y^{(i)}=0|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=0}=\\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} & 似然函数 \\\\ & \\log (\\ell(\\theta) )= －\\frac{1}{m} \\sum^m \\lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \\rgroup & 对数似然函数\\\\ &\\frac{\\partial J}{\\partial \\theta} = \\frac {1}{m} \\sum^m (y-g(x)) \\cdot x &对数似然函数求导\\end{align\*}$

## 一般回归

# 分类

## [支持向量机](http://127.0.0.1/?p=1430)

- 取值：
    - 将满足$latex \\theta\\cdot x>0 $的样本类别的输出值为1
    - 将满足$latex \\theta\\cdot x<0 $的样本类别的输出值为-1
- 损失函数
    - 正确分类的样本，$latex y\\theta\\cdot x>0$
    - 错误分类的样本,    $latex y\\theta\\cdot x<0$
- 损失函数的目标
    - 误分类的所有样品，到超平面的距离之和最小。
- 构造损失函数
    - 由于 $latex y\\theta\\cdot x<0$，对于每一个误分类的样本i，到超平面的距离是
        - $latex -\\frac{y^{(i)}\\theta\\cdot x^{(i)}}{||\\theta||\_2}$
            - 其中 $latex ||\\theta||\_2$为L2范数，也就是$latex \\theta$的各个值的平方和。
    - 我们假设所有误分类的的的集合为M，则所有误分类的样本到超平面的距离之和为
        - $latex -\\sum\\limits\_{x\_i\\in M}\\frac{y^{(i)}\\theta\\cdot x^{(i)}}{||\\theta||\_2}$
- 精简
    - 我们发现分子和分母都含有 $latex \\theta$ ,分子和分母存在固定的倍数关系，那么我们可以固定分子或者分母为1，然后求分子自己或者分母的导数的最小化作为损失函数。在感知器模型中，我们采取的是保留分子，精简化的感知器模型的损失函数如下：
        - $latex J(\\theta)=-\\sum\\limits\_{x\_i\\in M}y^{(i)}\\theta\\cdot x^{(i)}$

## [支持向量机](http://127.0.0.1/?p=1430)

# 聚类

## [K-MEANS均值聚类算法](http://127.0.0.1/?p=3041)
