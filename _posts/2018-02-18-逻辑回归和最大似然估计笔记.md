---
title: "逻辑回归和最大似然估计笔记"
date: "2018-02-18"
categories: 
  - "数学"
---

线性回归和逻辑回归是机器学习中的2种算法，分别用于预测和分类，

1. 线性回归得到有一个近似的参数，然后根据这个参数预测值。
2. 逻辑回归会判断某个值的概率。

例子： 判断一个电邮是否是垃圾电邮，我们需要分类器预测分类结果，记作1（是）或者0（不是），如果我们考虑0就是“不发生”，1就是“发生”，那么我们可以将分类任务理解成估计事件发生的概率p（这个是我们要求的），通过事件发生的概率的大小来达到分类的目的，因此我们需要使得预测结果，即概率值取值范围在 0 ~ 1 之间，很明显，线性回归的 $latex p=f\_{\\theta}(x) $ 的结果范围远远超过 0~1 之间。我们需要界定取值范围在 0~1 之间，然后找我们需要的函数。

1. 首先优势比代替概率 $latex odds= \\frac {p}{1-p} $ ，然后因为 $latex p \\in \[ 0 \\to 1 \] $  ，所以 $latex odds= \\frac {p}{1-p} $的计算结果就是从 $latex \[ 0 \\to \\infty \]$。
2. 然后因为线性回归的输出可能为负数，我们可以计算优势比的对数。 $latex \\eta =log(odds)=log \\frac {p}{1-p} = logit(p) $ ,logit(p)表示对p的一个关于log方面的方程，因为 $latex p \\in \[ 0 \\to 1 \] $ ，$latex e^0=1 $，在p越来越大，至少 $latex \\frac {p}{1-p} > 1 $的时候，这个结果可以在  $latex \[ 0 \\to \\infty \]$ ，而在p越来越少，至少 $latex \\frac {p}{1-p} < 1 $的时候，这个结果可以是$latex \[ -\\infty \\to 0 \]$  ,综合起来就是 $latex \[-\\infty \\to \\infty \] $ ，而这个分界线就是p=0.5的时候，这时候 $latex \\frac {p}{1-p}=\\frac{0.5}{1-0.5}=1 $

经过如上2步，我们可以去除分类问题对因变量值域的限制，如果概率=0，那么优势比=0，logit(p)的值就是 $latex -\\infty $ ,如果概率为1，那么优势比就是1，logit(p)的值就是 $latex \\infty $  ， 因此logit函数可以将 \[0,1\] 的概率值映射到整个实数空间，当概率小于0.5时，logit为负数，当概率大于0.5时，logit为正数，（这个正负数是假定要找的函数能实现的功能。）

我们可以采用线性回归对一一映射后的概率值进行线性拟合。即 $$ logit(p)=\\log(\\frac{p}{1-p})=\\eta=f\_\\theta(x)=\\theta^T\\cdot x $$

对这个公式两边都进行e的次方

\\\[  \\begin{align\*} e^{\\log(\\frac{p}{1-p})}&=e^\\eta=e^{\\theta^T\\cdot x} \\\\  \\frac{p}{1-p}&=e^\\eta=e^{\\theta^T\\cdot x} \\\\ p &= (1-p)e^{\\theta^T\\cdot x} \\\\ p&=\\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}}  \\end{align\*}\\\]

\\\[ \\begin{align\*}  1-p&=1-\\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}} \\\\ &=\\frac{1+e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}} - \\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}} \\\\  & =\\frac{1+e^{\\theta^T\\cdot x}-e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}} \\\\  & =\\frac{1}{1+e^{\\theta^T\\cdot x}} \\end{align\*} \\\]

sigmoid函数 $$ g(x)=\\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}}  $$

logistic回归表达式：

- $latex P(y=1 | X) = \\frac {e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}}$
- $latex P(y=0 | X) = \\frac {1}{1+e^{\\theta^{T} \\cdot x}}$
- P(y=1 | X) 表示给定参数X，求y=1时的概率。
- $latex \\theta $是未知参数，

现在就变成给定一组数据（X）和参数（y）待定的情况下，如何评估模型参数的问题。

最大似然估计（_Maximum Likelihood Method_）就是其中的一种，最大似然估计是建立在各样品间相互独立且样品满足随机抽样（可代表总体分布）下的估计方法，它的核心思想是如果现有样品可以代表总体，那么最大似然估计就是找到一组参数使得出现现有样品的可能性最大，即从统计学角度需要使得所有观测样品的联合概率最大化，又因样品间是相互独立的，所有所有观测样品的联合概率可以写成各样品出现概率的连乘积。如下公式。

$$ \\prod^m\_{i=1}\\underbrace{P(y^{(i)}=1|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=1} \\cdot \\underbrace{P(y^{(i)}=0|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=0}=\\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} $$

- i 是样品数量，$latex \\prod ^m\_{i+1}$，连乘积。
- 当y(i)=1时，上边公式等于 $latex P(y^{(i)}=1|x^{(i)})^{1} \\cdot P(y^{(i)}=0|x^{(i)})^{1-1} {\\color {red} =\\ \\  } P(y^{(i)}=1|x^{(i)}) $
- 当y(i)=0时，上边公式等于 $latex P(y^{(i)}=1|x^{(i)})^{0} \\cdot P(y^{(i)}=0|x^{(i)})^{1-0} {\\color {red}  =\\ \\  } P(y^{(i)}=0|x^{(i)})$
- 这个是所有样品边际分布概率的连乘积
    - 边际分布概率指的是，这个概率分布区间是 \[ 0 , 1 \] ，边际就是边际。
- 这个指数其实也是可以用示性函数来表示的，但是这种表示方便更简单点，毕竟只是单纯的算术运算。

最大似然函数的目标是求得使得似然函数最大的参数 $latex \\theta $ 的组合，理论上我们可以从梯度上升法求得最大值，只是这个不是凸函数，比如如下的

[![](images/nonconvex.png)](http://127.0.0.1/wp-content/uploads/2018/02/nonconvex.png)

这样就不能用梯度上升法来求了，

如下是原先的公式

$$ \\ell(\\theta) = \\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} $$

对两边都求log

\\\[ \\begin{align\*}  \\log (\\ell(\\theta) )&= \\log (\\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} ) \\\\ &= \\sum^m \\lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \\rgroup \\end{align\*} \\\]

- 比如log10(100\*1000)=log10(100)+log10(1000)，这个会将乘积问题转化成加法问题。
- 而 $latex P(y^{(i)}=1|x^{(i)})^{y(i)} $ ，这个表示给定x，求y=1时的概率，然后原先方程是y=g(x)，这里将这个代入，就是如上的形式，
- 比如 $latex log10(10^2) = 2 \* log10(10) $，如上的降幂就是这么来的。
- 加上log，是凸函数了。

求对数似然函数的最大值，

$$ cost=J(\\theta)=log(\\ell(\\theta))＝\\frac{1}{m} \\sum^m\_{i=1} \\lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \\rgroup $$

- g(x)为根据某个参数的预测值，而y(i)为实际值。
- 当 $latex y^{(i)}=1 , g(x) =1 $时，这样就是 1\*log(1)+(1-1)\*log(1-1) = 1 。
    - 而当 $latex y^{(i)}=1 , g(x) \\to 0 $时，结果就是1\*log (近似0) + 0\* log (1-g(x)) ,当log越来越小，则log(g(x))会在负值方向上越来越小，前面还有一个负号，总体结果就是越来越大。

已知损失函数了，求最小值，梯度下降法，对参数$latex \\theta $求导，这里设 $latex y^{(i)}=y $，简单点。

\\\[ \\begin{align\*} \\frac{\\partial J}{\\partial \\theta} & = \\frac {1}{m} \\sum\_{i=1}^m ( \\frac{y}{g(x)} \\cdot g’(x) - \\frac{1-y}{1-g(x)}\\cdot g’(x)) \\\\ & = \\frac {1}{m} \\sum^m( \\frac{y-yg(x)-g(x)+yg(x)}{g(x)(1-g(x))} \\cdot g’(x) ) \\\\ & = \\frac {1}{m}\\sum^m \\frac{y-g(x)}{g(x)(1-g(x))} \\cdot (\\frac{x e^{\\theta x}}{(1+e^{\\theta x})^2}) \\\\ & = \\frac {1}{m} \\sum^m \\frac{y-g(x)}{g(x)(1-g(x))} \\cdot \\frac{e^{\\theta x}}{1+e^{\\theta x}} \\cdot \\frac{1}{1+e^{\\theta x}} \\cdot x \\\\ & = \\frac {1}{m} \\sum^m \\frac{y-g(x)}{g(x)(1-g(x))} \\cdot g(x) \\cdot (1-g(x)) \\cdot x \\\\ & = \\frac {1}{m} \\sum^m (y-g(x)) \\cdot x \\\\ \\end{align\*} \\\]

- 解$latex log(1+x^2)$的导数为$latex \\frac{2x}{1+x^2} $，上边 对 $latex \\log{g(x)}$ 求导就是 $latex \\frac{g'(x)}{g(x)} $，g'(x)表示g(x)的导数。
    - 就像对函数 $latex f=x^2 $来说，求导数，就职指数变成乘数，原先的次幂降低一级。结果是 2x
- 而g'(x)计算方法如下：
    - $latex g(x)=\\frac{e^{\\theta^T\\cdot x}}{1+e^{\\theta^T\\cdot x}}  $
    - 根据导数除法 ： \\\[ \\begin{align\*} g'(x) &= \\frac{ (e^{\\theta x})'(1+e^{\\theta x})-e^{\\theta x}(1+e^{\\theta x})'}{(1+e^{\\theta x})^2} \\\\ &=\\frac {x e^{\\theta x}+x e^{\\theta x}e^{\\theta x}-xe^{\\theta x}e^{\\theta x}}{(1+e^{\\theta x})^2} \\\\ &=\\frac{x e^{\\theta x}}{(1+e^{\\theta x})^2} \\end{align\*} \\\]

如上是求的是梯度上升法的梯度方向，迭代公式就是

$$ \\theta^j=\\theta^j+\\alpha (y^{(i)}-g\_{\\theta}(x^{(i)})) \\cdot x^{(i)} $$

梯度上升法和梯度下降法区别只是一个是加上梯度\*步长，一个是减去梯度\*步长，对于 $latex \\theta $的值，一个是上升，一个是下降，但是共同点是对于梯度，不管上升还是下降，梯度都是在减少的。梯度是斜率，正切值，当越来越接近山顶或者谷底的时候，斜率都是减少的。

总结：

1. 前提 ： 知道一堆x的样品值和y的实际值（0或者1）
2. 求：给出一个新的样品，求y是1的概率p，而0的概率就是 1-p

# 思考过程

1. 1. 优势比 ，概率的取值为\[0~1\]，而f(x)的值范围比这大的多。用优势比映射。
    2. 对数优势比，优势比的映射范围只是映射到0到正无穷，加上两边都求对数，映射范围就映射到了负无穷到正无穷。
    3. 似然函数 ， 边际概率的连乘积，连乘积最大的时候，说明这个f(x)的参数$latex \\theta $是最优的。
    4. 连乘积难算啊，用两边求对数的方法，将连乘积转化求和，且将 $latex P(y^{(i)}=1|x^{(i)})^{y(i)}$ 的次幂转化成乘法运算。
    5. 用梯度下降法求损失函数最小，要对这个对数似然函数求导。

\\\[ \\begin{align\*} & \\frac{p}{1-p}=f(x)=\\theta^T x & 优势比 \\\\ & \\log \\frac{p}{1-p} = f\_\\theta(x)=\\theta^T x & 对数优势比  \\\\ & \\prod^m\_{i=1}\\underbrace{P(y^{(i)}=1|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=1} \\cdot \\underbrace{P(y^{(i)}=0|x^{(i)})}\_{where \\, i\\in m \\, and \\, y^{(i)}=0}=\\prod^m\_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \\cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)} & 似然函数 \\\\ &  \\log (\\ell(\\theta) )= －\\frac{1}{m} \\sum^m \\lgroup y^{(i)} log(g(x)) + (1-y^{(i)})log(1-g(x)) \\rgroup & 对数似然函数\\\\ &\\frac{\\partial J}{\\partial \\theta} = \\frac {1}{m} \\sum^m (y-g(x)) \\cdot x &对数似然函数求导\\end{align\*} \\\]

 

参考 ：

- [机器学习－逻辑回归与最大似然估计](http://www.hanlongfei.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/08/05/mle/)
- [逻辑回归（Logistic Regression）](http://www.cnblogs.com/BYRans/p/4713624.html)
- [机器学习算法整理（二）梯度下降求解逻辑回归 python实现](http://www.cnblogs.com/douzujun/p/8370993.html)
