---
layout: post
title: "pytorch总结"
date: "2023-09-14"
categories: ["计算机语言", "Python"]
---

# 摘要

- 重要的数据类型Tensor（多维数组），跟numpy共享内存空间。
- 重要的操作是求导（autograd)，对tensor所有运算提供自动微分功能，也就是计算梯度，并且是 define-run-time 的，每次的结果可能不一样。
    - tensor变量.requires\_grad=True，表示追踪变量的所有操作。
    - .backward() ，计算所有的梯度，结果放在.grad中。
    - with torch.no\_grad(): 不计算梯度。

自动求导的例子

```python
import torch

# 这个是一个普通的自动求导的例子
x = torch.ones(2, 2, requires_grad=True) # 定义x是一个2*2的矩阵
y = x + 2                                # 简单的加法运算
z = y * y * 3                            # 稍微复杂的乘法运算, z=3*(x+2)²
out = z.mean()                           # 求平均
out.backward()                           # 反向传播
print(x.grad)                            # 梯度
```

输出结果是

```
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```

 

训练流程

- 定义网络
- 数据预处理
- 数据输入
- 计算网络的损失
- 反向传播，计算梯度。
- 更新网络的梯度，一个简单的更新规则是: weight = weight - learning\_rate \* gradient

代码示例

```python
import torch
import torch.nn as nn            # 网络
import torch.nn.functional as F  # 函数
import torch.optim as optim      # 优化器，方便进行更新参数的

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)

# 这里随机一个输入第一个1是批次然后其他的纬度是图片的通道，宽和高
_img_input = torch.randn(1, 1, 32, 32)            # 随机输入
target = torch.randn(10)                          # 随机值作为样例
target = target.view(1, -1)                       # 使target和output的shape相同
criterion = nn.MSELoss()                          # 损失函数
optimizer = optim.SGD(net.parameters(), lr=0.01)  # 创建一个优化器，方便更新参数的
# 计算
for i in range(100):
    optimizer.zero_grad()                             # 梯度缓存清零，不然反向传播会用到
    output = net(_img_input)                          # 根据输入计算结果
    loss = criterion(output, target)                  # 损失函数，
    loss.backward()                                   # 损失函数反向传播，会依次更新前面变量的导数。
    optimizer.step()                                  # 优化器根据倒数更新参数
    if i % 10 == 0:
        print(loss)
```

 

```
# 显示参数
[w, b] = model.parameters()
print (w.item(),b.item())
```

 

# torch.nn

## 容器

### torch.nn.Module

所有神经网络模块的基类

```
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
```

 

### torch.nn.Sequential

一个顺序容器

```
# Using Sequential to create a small model. When `model` is run,
# input will first be passed to `Conv2d(1,20,5)`. The output of
# `Conv2d(1,20,5)` will be used as the input to the first
# `ReLU`; the output of the first `ReLU` will become the input
# for `Conv2d(20,64,5)`. Finally, the output of
# `Conv2d(20,64,5)` will be used as input to the second `ReLU`
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Using Sequential with OrderedDict. This is functionally the
# same as the above code
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))
```

 

### torch.nn.ModuleList

在列表中保存子模块。

ModuleList 可以像常规 Python 列表一样进行索引，但它包含的模块已正确注册，并且对于所有 Module 方法都是可见的

```
class MyModule(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
```

 

### torch.nn.ModuleDict

在字典中保存子模块。

```
class MyModule(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.choices = nn.ModuleDict({
                'conv': nn.Conv2d(10, 10, 3),
                'pool': nn.MaxPool2d(3)
        })
        self.activations = nn.ModuleDict([
                ['lrelu', nn.LeakyReLU()],
                ['prelu', nn.PReLU()]
        ])

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x
```

### torch.nn.ParameterList

在列表中保存参数。

```
class MyModule(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ParameterList can act as an iterable, or be indexed using ints
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x
```

 

### torch.nn.ParameterDict

 

在字典中保存参数。

```
class MyModule(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.params = nn.ParameterDict({
                'left': nn.Parameter(torch.randn(5, 10)),
                'right': nn.Parameter(torch.randn(5, 10))
        })

    def forward(self, x, choice):
        x = self.params[choice].mm(x)
        return x
```

 

## 卷积层

### torch.nn.Conv1d

一维度卷积

```
torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

- **in\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 输入图像中的通道数
- **out\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 卷积产生的通道数
- **kernel\_size** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")) – 卷积核的大小
- **stride** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 卷积的步幅。默认值：1
- **padding** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)") _或_ [_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – 添加到输入两侧的填充。默认值：0
- **dilation** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 内核元素之间的间距。默认值：1
- **groups** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ _可选_) – 从输入通道到输出通道的阻塞连接数。默认值：1
- **bias** ([_bool_](https://docs.pythonlang.cn/3/library/functions.html#bool "(in Python v3.13)")_,_ _可选_) – 如果 `True`，则向输出添加可学习的偏差。默认值：`True`
- **padding\_mode** ([_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – `'zeros'`, `'reflect'`, `'replicate'` 或 `'circular'`。默认值：`'zeros'`

 

最简单的情况，输入是$ (N,C\_{in},L)$ ，输出是$ (N,C\_{out},L\_{out})$,其中N是批次，C表示通道数，L表示信号序列的长度

 

```
m = nn.Conv1d(16, 33, 3, stride=2)
input = torch.randn(20, 16, 50)
output = m(input)
```

 

## torch.nn.Conv2d

2维度卷积

```
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)[source]
```

 

- **in\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 输入图像中的通道数
- **out\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 卷积产生的通道数
- **kernel\_size** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")) – 卷积核的大小
- **stride** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 卷积的步长。 默认值：1
- **padding** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)") _或_ [_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – 添加到输入的所有四个边的填充。 默认值：0
- **dilation** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 内核元素之间的间距。 默认值：1
- **groups** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ _可选_) – 从输入通道到输出通道的阻塞连接数。 默认值：1
- **bias** ([_bool_](https://docs.pythonlang.cn/3/library/functions.html#bool "(in Python v3.13)")_,_ _可选_) – 如果 `True`，则向输出添加可学习的偏置。 默认值：`True`
- **padding\_mode** ([_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – `'zeros'`, `'reflect'`, `'replicate'` 或 `'circular'`。 默认值：`'zeros'`

 

最简单的情况，输入是$ (N,C\_{in},H,W)$,输出是$ (N,C\_{out},H\_{out},W\_{out})$

```
# With square kernels and equal stride
m = nn.Conv2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
input = torch.randn(20, 16, 50, 100)
output = m(input)
```

 

### torch.nn.Conv3d

```
torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

- **in\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 输入图像中的通道数
- **out\_channels** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")) – 卷积产生的通道数
- **kernel\_size** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")) – 卷积核的大小
- **stride** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 卷积的步幅。默认值：1
- **padding** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)") _或_ [_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – 添加到输入所有六个侧面的填充。默认值：0
- **dilation** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)") _或_ [_tuple_](https://docs.pythonlang.cn/3/library/stdtypes.html#tuple "(in Python v3.13)")_,_ _可选_) – 内核元素之间的间距。默认值：1
- **groups** ([_int_](https://docs.pythonlang.cn/3/library/functions.html#int "(in Python v3.13)")_,_ _可选_) – 从输入通道到输出通道的阻塞连接数。默认值：1
- **bias** ([_bool_](https://docs.pythonlang.cn/3/library/functions.html#bool "(in Python v3.13)")_,_ _可选_) – 如果为 `True`，则向输出添加可学习的偏差。默认值：`True`
- **padding\_mode** ([_str_](https://docs.pythonlang.cn/3/library/stdtypes.html#str "(in Python v3.13)")_,_ _可选_) – `'zeros'`, `'reflect'`, `'replicate'` 或 `'circular'`。默认值：`'zeros'`

 

```
# With square kernels and equal stride
m = nn.Conv3d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
input = torch.randn(20, 16, 10, 50, 100)
output = m(input)
```

 

### torch.nn.ConvTranspose1d

对由几个输入平面组成的输入图像应用 1D 转置卷积运算符。

此模块可以看作是 Conv1d 相对于其输入的梯度。它也称为分数步长卷积或反卷积（虽然它不是真正的反卷积操作，因为它不计算卷积的真逆）。有关更多信息，请参阅[此处](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)的可视化以及[反卷积网络](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)论文。

```
torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)
```

 

### torch.nn.ConvTranspose2d

```
torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)[source]
```

 

### torch.nn.ConvTranspose3d

对由若干输入平面组成的输入图像应用 3D 转置卷积运算符。转置卷积运算符将每个输入值与可学习的内核进行元素级乘法，并对来自所有输入特征平面的输出求和。

此模块可以看作是 Conv3d 相对于其输入的梯度。它也称为分数步长卷积或反卷积（尽管它不是实际的反卷积运算，因为它不计算卷积的真逆）

```
torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)[source]
```

 

## torch.nn.LazyConv1d

```
torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)[源代码]
```

一个 torch.nn.Conv1d 模块，具有 in\_channels 参数的延迟初始化，我理解的是以后初始化输入通道参数。

### torch.nn.LazyConv2d

```
torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

 

### torch.nn.LazyConv3d

```
torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)[源代码]
```

 

### LazyConvTranspose1d

```
torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)
```

一个 torch.nn.ConvTranspose1d 模块，具有 in\_channels 参数的延迟初始化。

 

### torch.nn.LazyConvTranspose2d

```
torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)[source]
```

 

### torch.nn.LazyConvTranspose3d

```
torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)[source]
```

 

## 池化层

<table style="border-collapse: collapse; width: 100%; height: 240px;"><tbody><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxPool1d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 1D 最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxUnpool1d</td><td style="width: 67.4603%; height: 24px;">计算 MaxPool1d 的部分逆运算。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxUnpool2d</td><td style="width: 67.4603%; height: 24px;">计算 MaxPool2d 的部分逆运算。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.MaxUnpool3d</td><td style="width: 67.4603%; height: 24px;">计算 MaxPool3d 的部分逆运算。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AvgPool1d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 1D 平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AvgPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AvgPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.FractionalMaxPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 分数最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.FractionalMaxPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 分数最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.LPPool1d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 1D 功率平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.LPPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 功率平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.LPPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 功率平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveMaxPool1d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 1D 自适应最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveMaxPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 自适应最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveMaxPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 自适应最大池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveAvgPool1d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 1D 自适应平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveAvgPool2d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 2D 自适应平均池化。</td></tr><tr style="height: 24px;"><td style="width: 32.5397%; height: 24px;">nn.AdaptiveAvgPool3d</td><td style="width: 67.4603%; height: 24px;">对由几个输入平面组成的输入信号应用 3D 自适应平均池化。</td></tr></tbody></table>

 

## 填充层

### 输入边界的反射量来填充

```
m = nn.ReflectionPad1d(2)
input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
input
m(input)
# using different paddings for different sides
m = nn.ReflectionPad1d((3, 1))
m(input)
```

 

- ReflectionPad1d
- ReflectionPad2d
- ReflectionPad3d

 

### 使用输入边界的复制来填充输入张量

```
m = nn.ReplicationPad1d(2)
input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
input
m(input)
# using different paddings for different sides
m = nn.ReplicationPad1d((3, 1))
m(input)
```

 

- nn.ReplicationPad1d
- nn.ReplicationPad2d
- nn.ReplicationPad3d

 

### 用零填充输入张量边界

```
m = nn.ZeroPad1d(2)
input = torch.randn(1, 2, 4)
input
m(input)
m = nn.ZeroPad1d(2)
input = torch.randn(1, 2, 3)
input
m(input)
# using different paddings for different sides
m = nn.ZeroPad1d((3, 1))
m(input)


```

- ZeroPad1d
- ZeroPad2d
- ZeroPad3d

 

### 用常数值填充输入张量边界

```
m = nn.ConstantPad1d(2, 3.5)
input = torch.randn(1, 2, 4)
input
m(input)
m = nn.ConstantPad1d(2, 3.5)
input = torch.randn(1, 2, 3)
input
m(input)
# using different paddings for different sides
m = nn.ConstantPad1d((3, 1), 3.5)
m(input)
```

 

- ConstantPad1d
- ConstantPad2d
- ConstantPad3d

 

### 使用输入边界的循环填充来填充输入张量

```
>>> m = nn.CircularPad1d(2)
>>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
>>> input
tensor([[[0., 1., 2., 3.],
         [4., 5., 6., 7.]]])
>>> m(input)
tensor([[[2., 3., 0., 1., 2., 3., 0., 1.],
         [6., 7., 4., 5., 6., 7., 4., 5.]]])
>>> # using different paddings for different sides
>>> m = nn.CircularPad1d((3, 1))
>>> m(input)
tensor([[[1., 2., 3., 0., 1., 2., 3., 0.],
         [5., 6., 7., 4., 5., 6., 7., 4.]]])
```

 

- nn.CircularPad1d
- nn.CircularPad2d
- nn.CircularPad3d

 

## 非线性激活

torch.nn.ELU

指数线性单元 (ELU) 函数。

$$ ELU(x)= \\begin{equation} \\left\\{ \\begin{array}{\*\*lr\*\*} x, & if x > 0 \\\\ \\alpha \* (exp(x)-1), &  if x \\leq 0 \\end{array} \\right. \\end{equation} $$

[![no img]](http://127.0.0.1/?attachment_id=5484)

### torch.nn.Hardshrink

逐元素地应用 Hard Shrinkage (Hardshrink) 函数。

```
torch.nn.Hardshrink(lambd=0.5)
```

 

$$ HardShrink(x)= \\begin{equation} \\left\\{ \\begin{array}{\*\*lr\*\*} x, & if x > \\lambda  \\\\ x, & if x < -\\lambda  \\\\ 0, & otherwise \\end{array} \\right. \\end{equation}  $$

[![no img]](http://127.0.0.1/?attachment_id=5491)

 

 

 

 

 

# 引用

- [PyTorch](https://pytorch.ac.cn/)
