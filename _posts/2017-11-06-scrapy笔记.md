---
layout: post
title: "scrapy入门教程"
date: "2017-11-06"
categories: ["计算机语言", "Python"]
---

# 整体架构

[![no img]](http://127.0.0.1/wp-content/uploads/2017/11/78TFWC4B4_BE_T8_VWU.png)

<table width="617"><tbody><tr><td width="203">引擎(Scrapy Engine)</td><td width="414">用来处理整个系统的数据流处理，触发事务。</td></tr><tr><td width="203">调度器(Scheduler)</td><td width="414">用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。</td></tr><tr><td width="203">下载器(Downloader)</td><td width="414">用于下载网页内容，并将网页内容返回给蜘蛛。</td></tr><tr><td width="203">蜘蛛(Spiders)</td><td width="414">蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。</td></tr><tr><td width="203">项目管道 (Item Pipeline)</td><td width="414">负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</td></tr><tr><td width="203"></td><td width="414"></td></tr><tr><td width="203">下载器中间件 (Downloader Middlewares)</td><td width="414">位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</td></tr><tr><td width="203">蜘蛛中间件 (Spider Middlewares)</td><td width="414">介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。</td></tr><tr><td width="203">调度中间件 (Scheduler Middlewares)</td><td width="414">介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</td></tr><tr><td width="203"></td><td width="414"></td></tr><tr><td colspan="2" width="617">爬取流程 上图绿线是数据流向，首先从初始URL开始，Scheduler会将其交给Downloader进行下载，下载之后会交给Spider进行分析，Spider分析出来的结果有两种：一种是需要进一步抓取的链接，例如之前分析的“下一页”的链接，这些东西会被传回Scheduler；另一种是需要保存的数据，它们则被送到Item Pipeline那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。</td></tr></tbody></table>

# 简单爬虫

做一个简单的爬虫如下4步就可以了

1. 创建项目
2. 定义提取的item
3. 写一个spider来爬取站点，并提取items
4. 写一个item pipline用来存储提取出来的items

下面分别具体说一下

## 创建项目

只是用如下的一个代码而已。

scrapy startproject tutorial

会在当前目录下创建如下的结构

T:\\tutorial>tree /f Folder PATH listing Volume serial number is 0006EFCF C86A:7C52 T:. │ scrapy.cfg │ └─tutorial │ items.py │ pipelines.py │ settings.py │ \_\_init\_\_.py │ └─spiders \_\_init\_\_.py

## 定义提取的item

类似python中的字典，不过他提供更多的保护，比如为定义的字段填充之类的。

 

 

```
from scrapy.item import Item, Field 
class DmozItem(Item):
    title = Field()
    link = Field()
    desc = Field()
```

这个类继承自Item，定义他的属性为Field对象。

## 在spider目录下编写爬虫

重要的部分

- name ： 爬虫的名字，启动爬虫的时候需要
- allowed\_domains  ： 允许的域名，只是在这里边爬。
- start\_urls ： 爬虫最开始爬的url列表
- parse : 爬虫的方法，这个可以编写很多个parse。

```
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector

from tutorial.items import DmozItem

class DmozSpider(BaseSpider):
   name = "dmoz"
   allowed_domains = ["dmoz.org"]
   start_urls = [
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
   ]

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       sites = hxs.path('//fieldset/ul/li')
       #sites = hxs.path('//ul/li')
       items = []
       for site in sites:
           item = DmozItem()
           item['title'] = site.path('a/text()').extract()
           item['link'] = site.path('a/@href').extract()
           item['desc'] = site.path('text()').extract()
           items.append(item)
       return items
```

### 提取item

scrapy用xpath来提取item

 

Selector对象有方法

- xpath ：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点.
- extract()：返回一个unicode字符串，该字符串为XPath选择器返回的数据
- re()： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来

## item pipeline

pipeline，意思是管道，流水线之类的，上边的spider提取出items，这个pipeline用来处理items。

定义pipeline很简单，就是一个类，继承自,然后实现如下方法

- process\_item(item,spider)
    - 这个方法必须实现返回一个item或者丢出DropItem异常。
    - 参数
        - item ：提取到的item
        - spider ： 被哪个爬虫提取的。
- open\_spider(spider) : 这个spider打开的时候调用
- close\_spider(spider):这个spider关闭的时候调用。

一个简单的爬虫例子

```
from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)

```

 

### ImagesPipeline

这个用来下载图片的 代码如下

```
import re
from scrapy.pipelines.images import ImagesPipeline
from scrapy import Request

class ImagesrenamePipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        # 循环每一张图片地址下载，若传过来的不是集合则无需循环直接yield
        for image_url in item['imgurl']:
            # meta里面的数据是从spider获取，然后通过meta传递给下面方法：file_path
            yield Request(image_url,meta={'name':item['imgname']})

    # 重命名，若不重写这函数，图片名为哈希，就是一串乱七八糟的名字
    def file_path(self, request, response=None, info=None):

        # 提取url前面名称作为图片名。
        image_guid = request.url.split('/')[-1]
        # 接收上面meta传递过来的图片名称
        name = request.meta['name']
        # 过滤windows字符串，不经过这么一个步骤，你会发现有乱码或无法下载
        name = re.sub(r'[？\\*|“<>:/]', '', name)
        # 分文件夹存储的关键：{0}对应着name；{1}对应着image_guid
        filename = u'{0}/{1}'.format(name, image_guid)
        return filename

```

 

这个爬虫的设置要加上如下的

```
ITEM_PIPELINES = {
   'ImagesRename.pipelines.ImagesrenamePipeline': 300,
}

# 设置图片存储目录
IMAGES_STORE = 'D:\ImagesRename'

```

 

# 设置爬虫
