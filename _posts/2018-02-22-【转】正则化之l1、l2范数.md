---
layout: post
title: "【转】正则化之L1、L2范数"
date: "2018-02-22"
categories: 
  - "数学"
---

**1\. 问题引入**

假设通过房子的size来预测房价price,我们建立回归方程来拟合数据。拟合的结果将会出现三种情况。

[![](http://s9.sinaimg.cn/mw690/002o3YiAzy72L2nkxeg48)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L2nkxeg48)欠拟合                                      合适的拟合                                过拟合                   

欠拟合：上方第一张图。我们获得拟合数据的这样一条直线，但是，实际上这并不是一个很好的模型。我们看看这些数据，很明显，随着房子面积增大，住房价格的变化趋于稳定或者说越往右越平缓。因此线性回归并没有很好拟合训练数据。我们把此类情况称为_欠拟合(underfitting)_**，**或者叫做_高偏差(bias)_**。**

过拟合​：上方第三个图。第三幅图中对于该数据集用一个四次多项式来拟合。因此在这里我们有五个参数θ0到θ4，这样我们同样可以拟合一条曲线，通过我们的五个训练样本，我们可以得到如右图的一条曲线。一方面，我们似乎对训练数据做了一个很好的拟合，因为这条曲线通过了所有的训练实例。但是，这实际上是一条很扭曲的曲线，它不停上下波动。因此，事实上我们并不认为它是一个预测房价的好模型。我们把这类情况叫做_过拟合(overfitting)，_也叫_高方差(variance)_**。**

解决过拟合方法：                                                                                                                         a**：**_尽量减少选取变量的数量_                                                                                                        具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。                                                                                                              b**：**_正则化_                                                                                                                                       正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。

**2\. L1范数**

​L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”。简而言之，即使参数值接近于零。在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n。如下：

[![](http://s1.sinaimg.cn/mw690/002o3YiAzy72LCRnMWI90)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72LCRnMWI90)

​同样计算导数得：

[![](http://s7.sinaimg.cn/mw690/002o3YiAzy72LCUYV4Ga6)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72LCUYV4Ga6)

上式中sgn(w)表示w的符号。那么权重w的更新规则为：​

[![](http://s2.sinaimg.cn/mw690/002o3YiAzy72LCWv22l41)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72LCWv22l41)

比原始的更新规则多出了η \* λ \* sgn(w)/n这一项。当w为正时，更新后的w变小。当w为负时，更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。

另外，上面没有提到一个问题，当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉η\*λ\*sgn(w)/n这一项，所以我们可以规定sgn(0)=0，这样就把w=0的情况也统一进来了。（在编程的时候，令sgn(0)=0,sgn(w>0)=1,sgn(w<0)=-1）

好，到这里，我们大概知道了L1可以实现稀疏，但我们会想呀，为什么要稀疏？让我们的参数稀疏有什么好处呢？这里扯两点：

_1）特征选择(Feature Selection)_

大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

_2）可解释性(Interpretability)_

另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1\*x1+w2\*x2+…+w1000\*x1000+b（当然了，为了让y限定在\[0,1\]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w\*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。

**3\. L2 范数**

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||^2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的。越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图）。

​在前面的介绍中，我们看到了如果用一个二次函数来拟合这些数据，那么它给了我们一个对数据很好的拟合。然而，如果我们用一个更高次的多项式去拟合，最终我们可能会得到一个曲线，它能很好地拟合训练集，但却并不是一个好的结果，因为它过度拟合了数据，因此，一般性并不是很好。让我们考虑下面的假设，我们想要加上**惩罚项**，从而使参数 θ3 和 θ4 足够的小。

[![](http://s11.sinaimg.cn/mw690/002o3YiAzy72L3toRMS1a)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L3toRMS1a) 

这里我的意思就是，上图的式子是我们的优化目标，也就是说我们需要尽量减少代价函数的均方误差。对于这个函数我们对它添加一些项，加上 1000 乘以 θ3 的平方，再加上 1000 乘以 θ4 的平方，

[![](http://s2.sinaimg.cn/mw690/002o3YiAzy72L3vR4Xf11)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L3vR4Xf11)

1000 只是我随便写的某个较大的数字而已。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让 θ3 和 θ4 尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 θ3 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 θ3 的值接近于 0，同样 θ4 的值也接近于 0，就像我们忽略了这两个值一样。如果我们做到这一点（ θ3 和 θ4 接近 0 ），那么我们将得到一个近似的二次函数。

[![](http://s13.sinaimg.cn/mw690/002o3YiAzy72L3wLHuAfc)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L3wLHuAfc)

因此，我们最终恰当地拟合了数据，我们所使用的正是二次函数加上一些非常小，贡献很小项（因为这些项的 θ3、 θ4 非常接近于0）。显然，这是一个更好的假设。

​更一般地，如果我们的所有参数值（上例中 θ0 到 θ4）都对应一个较小值的话（参数值比较小），那么往往我们会得到一个形式更简单的假设。在我们上面的例子中，我们惩罚的只是 θ3 和 θ4 ，使这两个值均接近于零，从而我们得到了一个更简单的假设，实际上这个假设大抵上是一个二次函数。

_实际上，这些参数的值越小，通常对应于越光滑的函数，也就是更加简单的函数。因此 就不易发生过拟合的问题。​_

来让我们看看具体的例子，对于房屋价格预测我们可能有上百种特征，与刚刚所讲的多项式例子不同，我们并不知道 θ3 和 θ4 是高阶多项式的项。所以，如果我们有一百个特征，我们并不知道如何选择关联度更好的参数，如何缩小参数的数目等等。

因此在正则化里，我们要做的事情，就是把减小我们的代价函数（例子中是线性回归的代价函数）所有的参数值，因为我们并不知道是哪一个或哪几个要去缩小。

因此，我们需要修改代价函数，在这后面添加一项，就像我们在方括号里的这项。当我们添加一个额外的正则化项的时候，我们收缩了每个参数。

[![](http://s6.sinaimg.cn/mw690/002o3YiAzy72L3LEqahf5)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L3LEqahf5) 

​顺便说一下，按照惯例，我们没有去惩罚 θ0，因此 θ0 的值是大的。这就是一个约定从 1 到 n 的求和，而不是从 0 到 n 的求和。但其实在实践中这只会有非常小的差异，无论你是否包括这 θ0 这项。但是按照惯例，通常情况下我们还是只从 θ1 到 θn 进行正则化。

[![](http://s16.sinaimg.cn/mw690/002o3YiAzy72L3Q7iVNaf)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002o3YiAzy72L3Q7iVNaf) 

_**λ 要做的就是控制惩罚项与均方差之间的平衡关系。**​_λ越大说明，参数被打压得越厉害，θ值越小

_​_

参考文章

​[机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)

[机器学习之正则化（Regularization）](http://doc.okbase.net/jianxinzhou/archive/111322.html)

[正则化方法：L1和L2 regularization、数据集扩增、dropout](http://www.mamicode.com/info-detail-517504.html)
