---
layout: post
title: "异常点检测算法总结"
date: "2020-05-12"
categories: ["机器学习", "理论支持"]
---

# 异常点检测算法常见类别

异常点检测的目的是找出数据集中和大多数数据不同的数据，常用的异常点检测算法一般分为三类。

- 　　　　第一类是基于统计学的方法来处理异常数据，这种方法一般会构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点。比如特征工程中的RobustScaler方法，在做数据特征值缩放的时候，它会利用数据特征的分位数分布，将数据根据分位数划分为多段，只取中间段来做缩放，比如只取25%分位数到75%分位数的数据做缩放。这样减小了异常数据的影响。
- 第二类是基于聚类的方法来做异常点检测。这个很好理解，由于大部分聚类算法是基于数据特征的分布来做的，通常如果我们聚类后发现某些聚类簇的数据样本量比其他簇少很多，而且这个簇里数据的特征均值分布之类的值和其他簇也差异很大，这些簇里的样本点大部分时候都是异常点。比如我之前讲到的BIRCH聚类算法原理和DBSCAN密度聚类算法都可以在聚类的同时做异常点的检测。
- 第三类是基于专门的异常点检测算法来做。这些算法不像聚类算法，检测异常点只是一个赠品，它们的目的就是专门检测异常点的，这类算法的代表是One Class SVM和Isolation Forest.

# One Class SVM算法

One Class SVM也是属于支持向量机大家族的，但是它和传统的基于监督学习的分类回归支持向量机不同，它是无监督学习的方法，也就是说，它不需要我们标记训练集的输出标签。

那么没有类别标签，我们如何寻找划分的超平面以及寻找支持向量呢？One Class SVM这个问题的解决思路有很多。这里只讲解一种特别的思路SVDD, 对于SVDD来说，我们期望所有不是异常的样本都是正类别，同时它采用一个超球体而不是一个超平面来做划分，该算法在特征空间中获得数据周围的球形边界，期望最小化这个超球体的体积，从而最小化异常点数据的影响。

假设产生的超球体参数为中心 $latex o$和对应的超球体半径 $latex r>0 $ ，超球体体积$latex V(r)$被最小化，中心$latex o$ 是支持向量的线性组合；跟传统SVM方法相似，可以要求所有训练数据点$latex x\_i $ 到中心的距离严格小于 $latex r$，但同时构造一个惩罚系数为 $latex C $的松弛变量 $latex ξ\_i$，优化问题如下所示：

\\\[ \\begin{equation} \\underbrace{min}\_{r,o}V(r) + C\\sum\\limits\_{i=1}^m\\xi\_i  \\\\ ||x\_i-o||\_2 \\leq r + \\xi\_i,\\;\\; i=1,2,...m  \\\\ \\xi\_i \\geq 0,\\;\\;i=1,2,...m \\end{equation}\\\]

和之前讲的支持向量机系列类似的求解方法，在采用拉格朗日对偶求解之后，可以判断新的数据点 z 是否在类内，如果z到中心的距离小于或者等于半径r,则不是异常点，如果在超球体以外，则是异常点。

这里的是松弛变量，和经典的SVM中的松弛变量的作用相同，它的作用就是，使得模型不会被个别极端的数据点给“破坏”了，想象一下，如果大多数的数据点都在一个小区域内，只有少数几个异常数据在离他们很远的地方，如果要找一个超球面把他们包住，这个超球面会很大，因为要包住那几个很远的点，这样就使得模型对离群点很敏感，说的通俗一点就是，那几个异常的点，虽然没法判断它是否真的是噪声数据，它是因为大数点都在一起，就少数几个不在这里，宁愿把那几个少数的数据点看成是异常的，以免模型为了迎合那几个少数的数据点会做出过大的牺牲，这就是所谓的过拟合（overfitting）。所以容忍一些不满足的硬性约束的数据点，给它一些弹性，同时又要包住training set中每个数据点都要满足约束，这样在后面才能使用Lagrange乘子法求解，因为Lagrange乘子法中是要包含约束条件的，如果你的数据都不满足约束条件，那就没法用了。注意松弛变量是带有下标 i 的，也就是说它是和每个数据点是有关系的，每个数据点都有对应的松弛变量，可以理解为：

对于每个数据点来说，那个超球面可以是不一样的，根据松弛变量来控制，如果松弛变量的值一样，那超球面就一样的，哪个C就是调节松弛变量的影响大小，说的通俗一点就是，给那些需要松弛的数据点多少松弛空间，如果C很大的话，那么在cost function中，由松弛变量带来的cost就大，那么training的时候会把松弛变量调小，这样的结果就是不怎么容忍那些离群点，硬是要把他们包起来，反之如果C比较小，那会给离群点较大的弹性，使得它们可以不被包含进行。现在就可以明白上面图为什么并没把点全都包住了。

下面展示两张图，第一张是C较小时候的情形，第二张图是C较大时的情形：

[![no img]](http://127.0.0.1/?attachment_id=3405)

 

在sklearn中，我们可以用svm包里面的OneClassSVM来做异常点检测。OneClassSVM也支持核函数，所以普通SVM里面的调参思路在这里也适用

## OneClass 与二分类，多分类的区别

如果将分类算法进行划分，根据类别个数的不同可以分为单分类，二分类，多分类。常见的分类算法主要解决二分类和多分类问题，预测一封邮件是否是垃圾邮件是一个典型的二分类问题，手写体识别是一个典型的多分类问题，这些算法并不能很好的应用在单分类上，但是单分类问题在工业界广泛存在，由于每个企业刻画用户的数据都是有限的，很多二分类问题很难找到负样本，即使用一些排除法筛选出负样本，负样本也会不纯，不能保证负样本中没有正样本。所以在只能定义正样本不能定义负样本的场景中，使用单分类算法更合适。

单分类算法只关注与样本的相似或者匹配程度，对于未知的部分不妄下结论。

典型的二类问题：识别邮件是否是垃圾邮件，一类“是”，一类“不是”。

典型的多类问题：人脸识别，每个人对应的脸就是一个类，然后把待识别的脸分到对应的类去。

而OneClassClassification，它只有一个类，属于该类就返回结果“是”，不属于就返回结果“不是”。

其区别就是在二分类问题中，训练集中就由两个类的样本组成，训练出的模型是一个二分类模型；而OneClassClassification中的训练样本只有一类，因此训练出的分类器将不属于该类的所有其他样本判别为“不是”即可，而不是由于属于另一类才返回“不是”的结果。

现实场景中的OneCLassClassification例子：现在有一堆某商品的历史销售数据，记录着买该产品的用户信息，此外还有一些没有购买过该产品的用户信息，想通过二分类来预测他们是否会买该产品，也就是两个类，一类是“买”，一类是“不买”。当我们要开始训练二分类器的时候问题来了，一般来说没买的用户数会远远大于已经买了的用户数，当将数据不均衡的正负样本投入训练时，训练出的分类器会有较大的bisa（偏向值）。因此，这时候就可以使用OneClassClassification 方法来解决，即训练集中只有已经买过该产品的用户数据，在识别一个新用户是否会买该产品时，识别结果就是“会”或者“不会”。

# Isolation Forest算法

## iTree

提到森林，自然少不了树，毕竟森林都是由树构成的，看Isolation Forest（简称iForest）前，我们先来看看Isolation Tree（简称iTree）是怎么构成的，iTree是一种随机二叉树，每个节点要么有两个女儿，要么就是叶子节点，一个孩子都没有。给定一堆数据集D，这里D的所有属性都是连续型的变量，iTree的构成过程如下：

- 　　随机选择一个属性Attr；
- 随机选择该属性的一个值Value；
- 根据Attr对每条记录进行分类，把Attr小于Value的记录放在左女儿，把大于等于Value的记录放在右孩子；
- 然后递归的构造左女儿和右女儿，直到满足以下条件：
- 传入的数据集只有一条记录或者多条一样的记录；
- 树的高度达到了限定高度；

[![no img]](http://127.0.0.1/?attachment_id=3408)

iTree构建好了后，就可以对数据进行预测啦，预测的过程就是把测试记录在iTree上走一下，看测试记录落在哪个叶子节点。iTree能有效检测异常的假设是：异常点一般都是非常稀有的，在iTree中会很快被划分到叶子节点，因此可以用叶子节点到根节点的路径h(x)长度来判断一条记录x是否是异常点；对于一个包含n条记录的数据集，其构造的树的高度最小值为log(n)，最大值为n-1，论文提到说用log(n)和n-1归一化不能保证有界和不方便比较，用一个稍微复杂一点的归一化公式：

$$ s(x,n) = 2^{(-\\frac{h(x)}{c(n)})} $$

$$ c(n) = 2H(n − 1) − (2(n − 1)/n), 其中 H(k) = ln(k) + \\xi，\\xi为欧拉常数 $$

$latex s(x,n)$就是记录x在由n个样本的训练数据构成的iTree的异常指数，$latex s(x,n)$

)

取值范围为\[0,1\]，越接近1表示是异常点的可能性高，越接近0表示是正常点的可能性比较高，如果大部分的训练样本的s(x,n)都接近于0.5，说明整个数据集都没有明显的异常值。

随机选属性，随机选属性值，一棵树这么随便搞肯定是不靠谱，但是把多棵树结合起来就变强大了；

## **iForest

**

iTree搞明白了，我们现在来看看iForest是怎么构造的，给定一个包含n条记录的数据集D，如何构造一个iForest。iForest和Random Forest的方法有些类似，都是随机采样一一部分数据集去构造每一棵树，保证不同树之间的差异性，不过iForest与RF不同，采样的数据量Psi不需要等于n，可以远远小于n，论文中提到采样大小超过256效果就提升不大了，明确越大还会造成计算时间的上的浪费，为什么不像其他算法一样，数据越多效果越好呢，可以看看下面这两个个图

[![no img]](http://127.0.0.1/?attachment_id=3410)

左边是元素数据，右边是采样了数据，蓝色是正常样本，红色是异常样本。可以看到，在采样之前，正常样本和异常样本出现重叠，因此很难分开，但我们采样之和，异常样本和正常样本可以明显的分开。

除了限制采样大小以外，还要给每棵iTree设置最大高度$latex l=ceiling(log\_2^\\Psi)  $，这是因为异常数据记录都比较少，其路径长度也比较低，而我们也只需要把正常记录和异常记录区分开来，因此只需要关心低于平均高度的部分就好，这样算法效率更高，不过这样调整了后，后面可以看到计算$latex h(x) $需要一点点改进，先看iForest的伪代码：

[![no img]](http://127.0.0.1/?attachment_id=3411)

 

IForest构造好后，对测试进行预测时，需要进行综合每棵树的结果，于是

$$ s(x,n) = 2^{(-\\frac{E(h(x))}{c(n)})} $$

E(h(x))表示记录x在每棵树的高度均值，另外h(x)计算需要改进，在生成叶节点时，算法记录了叶节点包含的记录数量，这时候要用这个数量SizeSize估计一下平均高度，h(x)的计算方法如下：

# 引用

- [异常点检测算法小结](https://www.cnblogs.com/pinard/p/9314198.html)
- [Python机器学习笔记：One Class SVM](https://www.cnblogs.com/wj-1314/p/10701708.html)
- [异常检测算法--Isolation Forest](https://www.cnblogs.com/fengfenggirl/p/iForest.html)
