---
title: "机器学习中的范数规范化笔记"
date: "2018-02-22"
categories: 
  - "数学"
---

机器学习2个目标：

- 训练误差小。
- 准测预测新的样品。

机器学习可以看作最小化如下函数 $$ w^\* = \\underbrace{arg min}\_{w} \\sum\_{i} L(y\_i,f(w,x)) + \\lambda\\Omega (w) $$

- $latex L(y\_i,f(w,x))$ ，是衡量样品预测值和真实值之间的误差。损失函数。
    
    - - Square loss ：平方和最小，最小二乘法。
        - Hinge Loss ： SVM
        - exp-Loss : Boosting
    
    - log-Loss : Logistic Regression ,逻辑回归就是用这个判断，概率累积后对数。
- $latex \\Omega (w)$ ，是对 w 参数的规范化函数，约束我们的模型尽量的简单。

Lo范数和L1范数 ：

- L0范数。
    - L0范数指的是向量中非零的个数。
        - 如果我们用L0范数来规范化参数矩阵w的话，就是希望w的大部分元素都是0，换句话说就是让参数w是稀疏的。
- L1范数：
    - L1范数指的是向量中各个元素的绝对值之和，叫“稀疏规则算子”，（Lasso regularization），

参数稀疏的好处

1. 特征选择 （Feature Selection）
    1. 一般来说，$latex x\_i $的大部分元素（也就是特征）都是和最终的输出$latex y\_i $没有关系或者不提供任何信息的，在最小化目标函数的时候考虑 $latex y\_i $的额外特征，虽然可以训练更小的训练误差，但在预测新的样品时，这些没用的信息反而会被考虑，从而干扰了对正确 $latex y\_i $ 的预测。
    2. 规则稀疏化算子的引入就是为了完成特征自动选择，它会去掉那些没有信息的特征，也就是把这些特征的权重重置为零。
2. 可解释性 （Interpretability）
    1. 模型更容易解释。
        1. 例子：比如患某种病的概率为y，我们收集到的模型是1000维的，也就是我们需要寻找这1000维元素到底怎么影响这种病的概率的，我们假设有个回归模型 $latex y=w\_1\*x\_1+w\_2\*x\_2+w\_{1000}\*x\_{1000} +b $ ,当然，因为 $latex y \\in \[ 0 , 1\] $ ,得用逻辑回归之类的，通过学习，如果最后学习到只有5个非零的w参数，我们有理由相信，这些对应的特征在患病上影响巨大，或者说，患不患这种病只与这5种因素有关，那就好分析多了。

L2范数，$latex ||w||\_2 $ ,在回归里，岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”，

- 定义 ： 指的是向量各个元素的平方和，然后求平方根。
- 意义 ：
    -  我们要使得L2范数的规则项最小，可以使得w的每个元素都很小，都接近0，但和L1范数不同，它不会让它等于0.
        - 越小的参数，模型越简单，参数很小，多项式的某些分量就很小，相当于减少了参数。
- 好处 ：
    - 防止过拟合。
    - 优化计算。
        - 优化2大难题
            - 局部最小值，而我们要找的是全局最小值，如果局部最小值太多，容易陷入局部不能自拔。
            - ill-condition病态问题 ，比如方程AX=b，如果A或者b稍微改变一下，值得X的解发生很大的改变，那么这个方程组系统就是 ill-condition病态的，反之是 well-condition 。
                - 如下的图片，![](http://img.blog.csdn.net/20140504122435031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
                - 咱们先看左边的那个。第一行假设是我们的AX=b，第二行我们稍微改变下b，得到的x和没改变前的差别很大，看到吧。第三行我们稍微改变下系数矩阵A，可以看到结果的变化也很大。换句话来说，这个系统的解对系数矩阵A或者b太敏感了。又因为一般我们的系数矩阵A和b是从实验数据里面估计得到的，所以它是存在误差的，如果我们的系统对这个误差是可以容忍的就还好，但系统对这个误差太敏感了，以至于我们的解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是ill-conditioned病态的，不正常的，不稳定的，有问题的，哈哈。这清楚了吧。右边那个就叫well-condition的系统了。
                - condition number ： 拿来衡量ill-condition系统的可信度的
                    - condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。
                    - condition number值小的就是well-conditioned的，大的就是ill-conditioned的。
                    - 计算 ： 如果方阵A是非奇异的，那么A的condition number 就是 $latex k(A)=||A||  ||A^{-1}|| $
                    - 解释 ： 也就是矩阵A的norm乘以它的逆的norm ，
                    - 结论 ，对于AX=b  \\\[ \\frac{||\\triangle x||}{||x||} \\leq||A||\\cdot||A^{-1}||\\cdot\\frac{||\\triangle b||}{||b||} \\\\ \\frac{||\\triangle x||}{||x||} \\leq K(A) \\cdot\\frac{||\\triangle b||}{||b||}  \\\\ \\frac{||\\triangle x||}{||x+\\triangle x||} \\leq K(A) \\cdot\\frac{||\\triangle A||}{||A||} \\\]

过拟合，比如下图，

![](http://img.blog.csdn.net/20140504122353812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

- 第一个图是欠拟合，因为看起来随着size增大，price增幅是减少的，而一个直线的增幅是不变的。
- 第二个不错。
- 第三个过拟合了，这个拟合到所有的历史数据了，误差是最小的，但是，对新的数据，误差就会非常大。

我们常常将原始数据集分成三部分

- training data ：训练数据，计算梯度，更新权重。
- validation data ： 确认数据，避免过拟合的，确定一些超参数
- testing data ： 测试数据，仅仅用来判断网络的好坏。

避免过拟合的方法很多：

- early stopping
- data augmentation ： 数据集递增。
- regularization ： 正则化
    - L1
    - L2 ： 权重衰减（weight decay ）
- dropout

# L2 regularization （权重衰减）

1. 公式 ： 就是在代价函数后边再加上一个正则化项 。$$ C=C\_0+\\frac{\\lambda}{2n}\\sum\_{w}w^2 $$
    - $latex C\_0 $ 代价原始的代价函数。
    - $latex frac{\\lambda}{2n}\\sum\_{w}w^2 $ 正则化项，所有参数的平方和除以训练集的样品大小n
        - $latex \\lambda $ : 正则项系数，权衡正则项与 $latex C\_0 $项目的比重
        - $latex \\sum\_{w}w^2$ : 所有参数的平方和。
        - $latex \\frac{1}{2} $ ： 求导的时候抵消掉而已。
2. 原理：
    - 先求导 分别对w和b求偏导： $$ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C\_0}{\\partial w} + \\frac{\\lambda}{n} w $$ $$ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C\_0}{\\partial b} $$
        - 可以看到L2对b的更新没有影响
        - 但对w的更新有影响。
    - 原先w更新可以写成 $$ w =w - \\eta \\frac{\\partial C\_0}{\\partial w}$$
        - $latex \\eta $ 为步长。
    - 现在w更新得写成 $$ w =w - \\eta \\frac{\\partial C\_0}{\\partial w} - \\eta  \\frac{\\lambda}{n} w $$  $$ (1-\\frac{\\eta \\lambda}{n} )w - \\eta \\frac{\\partial C\_0}{\\partial w} $$
        - 在不使用正则化前，w的系数是1，而当前w的系数是 $latex (1-\\frac{\\eta \\lambda}{n})$
            - 这三个项，$latex \\lambda ,\\eta , n$都是正数，则 $latex \\frac{\\eta \\lambda}{n}$的结果会是一个很小的正数，一般会小于1，作用是减少w，这个就是权重衰减的由来。当然考虑到后面的导数项，w最终的值可能增大也可能减小。

# L1 regularization ，L1 正则化

1. 公式 ： 在原始代价函数后边加上L1正则化项，即所有权重w的绝对值的和，再乘以什么什么。$$ C = C\_0 + \\frac{\\lambda}{n} \\sum\_w |w| $$
2. 原理 ：
    - 先求导数 ，只求对w的偏导吧，b的偏导应该还是不变的。$$ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C\_0}{\\partial w} + \\frac{\\lambda}{n}sgn(w) $$
        - sgn(w) : 表示w的符号。这个是对w求偏导，对|w|的这个绝对值就偏导，结果就是得到一个正负号。
    - w的更新就是 $$ w = w - \\frac{\\lambda}{n}sgn(w) - \\eta \\frac{\\partial C\_0}{\\partial w} $$
        - 可以看到多了 $latex - \\frac{\\lambda}{n}sgn(w)$这项
            - 当w为正的时候，更新后的w会减少。
            - 而当w为负的时候，更新后的w会增加。
            - 使得w尽量往0上靠，尽可能的为0 .
        - 另外，上面没有提到一个问题，当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉η\*λ\*sgn(w)/n这一项，所以我们可以规定sgn(0)=0，这样就把w=0的情况也统一进来了。（在编程的时候，令sgn(0)=0,sgn(w>0)=1,sgn(w<0)=-1）
3. 推导 ：
    - 我们先修改损失函数公式，让其简单点 $$ C = C\_0 + \\alpha \\omega ( \\theta ) $$
        - $latex \\alpha$ 是超参数，用来调整正则项与 $latex C\_0$ 的比重的，
            - 当这个为0的时候，相当于没有调整。
            - 当这个用很大的数字的时候，相当于惩罚数值大的参数。
    - 将 $latex \\Omega ( \\theta ) = ||w|| $代入，得到 $$ C = C\_0 + \\alpha ||w|| $$
    - 我们的目的是求得使目标函数w求得最小值，上边公式对w求导 $$ \\nabla\_w C = \\nabla C\_0 + \\alpha \\cdot sign(w) $$
        - 若w>0 , 则sign(w)>0
        - 若w<0 , 则sign(w)<0
        - 若w=0 , 则sign(w)=0
