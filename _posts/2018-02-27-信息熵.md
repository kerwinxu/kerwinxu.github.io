---
layout: post
title: "信息熵"
date: "2018-02-27"
categories: 
  - "数学"
---

### 信息熵

- 熵是衡量系统混乱程度的一个指标，熵越大，表示对应的系统越混乱。
- 定义： 假设目标集合S中有n个样本，第k个样本所占比例为pk (k =1,2,3...n)，也就是概率啦，则S的信息熵为 $$Ent(S)=-\\sum\_{k=1}^{n}p\_k \\log\_2{p\_k}$$
    - 请注意这个概率范围在0~1之间的，所以log这个范围的数，是负数，所以前面有个负号，负负得正嘛。
    - 理解：
        - 信息量应该跟概率有关，概率越小的，信息越大，而概率越大的，信息越小，比如太阳从东边升起，特定发生，信息量最小。
        - 两个不相关的事件x和y，我们观察到的两个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和。即 h(x,y)=h(x)+h(y)
            - 而这个，对数形式的相乘能够得到相加形式。
        - 还应该跟数量相关吧。
- 几个熵区别：讨论下二元随机变量的情况，为了直观解释和记忆，先给出如下图解：![no img]
    - 联合熵，代表X、Y同时发生的不确定性：$$ H(X,Y)=-\\sum \\limits\_{x\\in{X},y\\in{Y}}^{X,Y}p(x,y)\\log\_2{(x,y)}$$
    - 条件熵，代表在已知一个变量，另一个变量发生所新增的不确定性： $$ H(X|Y)=H(X,Y)-H(X)=-\\sum\\limits\_{x\\in{X},y\\in{Y}}^{X,Y}p(x,y)\\log\_2{(x|y)}$$
    - 交互信息 $$I(X,Y)=H(Y)-H(Y|X)=H(Y)+H(X)\\cdot{H(X,Y)}=-\\sum \\limits\_{x\\in{X},y\\in{Y}}^{X,Y}p(x,y)log\_2{\\frac{p(x,y)}{p(x)p(y)}}$$
        - 比如我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0.
            - 样本D的熵为： $ H(D) = -(\\frac{9}{15}log\_2\\frac{9}{15} + \\frac{6}{15}log\_2\\frac{6}{15}) = 0.971$
            - 样本D在特征下的条件熵为： \\\[ H(D|A) = \\frac{5}{15}H(D1) + \\frac{5}{15}H(D2) + \\frac{5}{15}H(D3)\\\\= -\\frac{5}{15}(\\frac{3}{5}log\_2\\frac{3}{5} + \\frac{2}{5}log\_2\\frac{2}{5}) - \\frac{5}{15}(\\frac{2}{5}log\_2\\frac{2}{5} + \\frac{3}{5}log\_2\\frac{3}{5}) -\\frac{5}{15}(\\frac{4}{5}log\_2\\frac{4}{5} + \\frac{1}{5}log\_2\\frac{1}{5}) = 0.888\\\]
            - 对应的信息增益为 $ I(D,A) = H(D) - H(D|A) = 0.083$
